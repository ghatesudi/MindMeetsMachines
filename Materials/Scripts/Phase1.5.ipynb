{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64933cd6-51d1-469e-bd85-8d21d8926950",
   "metadata": {},
   "source": [
    "### Phase A ‚Äî Concept Presence/Absence Matrix (FRD 6.1.1‚Äì6.1.4)\n",
    "- This cell constructs a **binary matrix** indicating the presence (1) or absence (0) of each concept across all non-subgroup AI and Human arms for each disease.  \n",
    "- Each row represents a unique concept, and columns represent different arms, capturing which workflow included each concept.  \n",
    "- The output CSV (`<Disease>_arm_matrix.csv`) contains both **concept metadata** and the **arm-wise binary indicators**, enabling subsequent similarity metrics, overlap calculations, and visualizations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38c906-0c05-45ff-8c66-f828614f8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_arm_name(folder_name):\n",
    "    \"\"\"\n",
    "    Extracts a clean, standardized arm name from a folder name.\n",
    "    Skips subgroup folders like [S01], [S02].\n",
    "    \"\"\"\n",
    "    # üö´ Skip subgroup folders (e.g., [H1][S01], [AI1][S02])\n",
    "    if re.search(r\"\\[S\\d+\\]\", folder_name, re.IGNORECASE):\n",
    "        return None\n",
    "\n",
    "    name = folder_name.strip().replace(\"Concept Set\", \"\").strip()\n",
    "    match = re.search(r\"(AI\\d+|H1|HUMAN|Clinician|Reviewer|Manual|Expert)\", name, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    else:\n",
    "        parts = name.split()\n",
    "        return parts[-1].title() if parts else name\n",
    "\n",
    "\n",
    "def build_concept_matrix(disease_folder):\n",
    "    arm_frames = []\n",
    "    arm_names = []\n",
    "    file_info = []\n",
    "\n",
    "    print(f\"\\nüîç Processing disease folder: {os.path.basename(disease_folder)}\")\n",
    "\n",
    "    for subfolder in os.listdir(disease_folder):\n",
    "        subfolder_path = os.path.join(disease_folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            csv_path = os.path.join(subfolder_path, 'includedConcepts.csv')\n",
    "            if not os.path.exists(csv_path):\n",
    "                continue\n",
    "\n",
    "            arm_name = clean_arm_name(subfolder)\n",
    "            if arm_name is None:\n",
    "                print(f\"   ‚è≠Ô∏è Skipping subgroup folder: {subfolder}\")\n",
    "                continue\n",
    "\n",
    "            # ‚úÖ READ only non-subgroup folders\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path, dtype=str)\n",
    "                n_concepts = df.shape[0]\n",
    "                print(f\"   üìÑ {arm_name:10s} ‚Üê {subfolder}/includedConcepts.csv ({n_concepts} rows)\")\n",
    "                df.columns = [c.strip() for c in df.columns]\n",
    "                df[\"Arm\"] = arm_name\n",
    "                arm_frames.append(df)\n",
    "                arm_names.append(arm_name)\n",
    "                file_info.append((arm_name, n_concepts))\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Error reading {csv_path}: {e}\")\n",
    "\n",
    "    if not arm_frames:\n",
    "        print(f\"‚ö†Ô∏è No includedConcepts.csv found in {disease_folder}\")\n",
    "        return None\n",
    "\n",
    "    all_concepts = pd.concat(arm_frames, ignore_index=True)\n",
    "    concept_id_col = [c for c in all_concepts.columns if \"conceptid\" in c.lower()]\n",
    "    if not concept_id_col:\n",
    "        raise ValueError(f\"No Concept ID column found in {disease_folder}\")\n",
    "    concept_col = concept_id_col[0]\n",
    "\n",
    "    matrix = all_concepts.pivot_table(\n",
    "        index=concept_col,\n",
    "        columns=\"Arm\",\n",
    "        values=\"conceptSetId\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "\n",
    "    drop_cols = [c for c in [\"Arm\", \"Name\", \"conceptSetName\"] if c in all_concepts.columns]\n",
    "    concept_meta = (\n",
    "        all_concepts\n",
    "        .drop(columns=drop_cols, errors=\"ignore\")\n",
    "        .drop_duplicates(subset=[concept_col])\n",
    "    )\n",
    "\n",
    "    merged = concept_meta.merge(matrix, on=concept_col, how=\"left\")\n",
    "    arm_cols = [col for col in merged.columns if col in arm_names]\n",
    "    merged[arm_cols] = (merged[arm_cols] > 0).astype(int)\n",
    "\n",
    "    disease_name = os.path.basename(disease_folder)\n",
    "    merged.insert(0, \"Disease\", disease_name)\n",
    "\n",
    "    output_name = f\"{disease_name}_arm_matrix.csv\"\n",
    "    merged.to_csv(output_name, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved: {output_name} ({merged.shape[0]} concepts, {len(arm_cols)} arms)\")\n",
    "    print(f\"   ‚îî‚îÄ Arms read: {', '.join([f'{a} ({n})' for a, n in file_info])}\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "ROOT_DIR = os.path.join('..', 'ConceptSets')   # adjust if needed\n",
    "disease_dirs = [\n",
    "    os.path.join(ROOT_DIR, d)\n",
    "    for d in os.listdir(ROOT_DIR)\n",
    "    if os.path.isdir(os.path.join(ROOT_DIR, d)) and re.match(r\"C\\d+_\", d)\n",
    "]\n",
    "\n",
    "for disease_dir in disease_dirs:\n",
    "    build_concept_matrix(disease_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b27812-2b81-4595-b87d-741b830b9d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Phase A ‚Äî Build Arm‚ÄìConcept Matrix per Disease\n",
    "# ==========================================================\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# --------------------------------------------\n",
    "# Robust ROOT_DIR setup (auto-detect)\n",
    "# --------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, '..', 'ConceptSets'))\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    alt_root = os.path.join(SCRIPT_DIR, 'ConceptSets')\n",
    "    if os.path.exists(alt_root):\n",
    "        ROOT_DIR = os.path.abspath(alt_root)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è ConceptSets folder not found.\")\n",
    "        ROOT_DIR = None\n",
    "\n",
    "OUT_DIR = os.path.join(SCRIPT_DIR, 'results', 'phaseA_arm_concept_matrix')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Restrict to target diseases\n",
    "# --------------------------------------------\n",
    "TARGET_DISEASES = {\n",
    "    'C01': 'SLE',\n",
    "    'C02': 'RheumatoidArthritis',\n",
    "    'C03': 'DiabetesMacularEdema',\n",
    "    'C04': 'DeepVeinThrombosis',\n",
    "    'C06': 'Uveitis',\n",
    "    'C07': 'SystemicSclerosis'\n",
    "}\n",
    "\n",
    "# --------------------------------------------\n",
    "# Helper functions\n",
    "# --------------------------------------------\n",
    "def strip_numeric_prefix(name):\n",
    "    \"\"\"Remove leading numeric prefixes like '0152-'.\"\"\"\n",
    "    return re.sub(r\"^[\\d\\-\\_]+\", \"\", name).strip()\n",
    "\n",
    "def clean_arm_name(folder_name):\n",
    "    \"\"\"\n",
    "    Extracts a standardized arm label (AI1, HUMAN, etc.)\n",
    "    Skips subgroup folders like [S01].\n",
    "    \"\"\"\n",
    "    if re.search(r\"\\[S\\d+\\]\", folder_name, re.IGNORECASE):\n",
    "        return None\n",
    "    match = re.search(r\"(AI\\d+|H\\d+|Human|Clinician|Reviewer|Manual|Expert)\", folder_name, re.IGNORECASE)\n",
    "    if match:\n",
    "        arm = match.group(1).upper()\n",
    "        if arm.startswith(\"H\"):\n",
    "            arm = \"HUMAN\"\n",
    "        return arm\n",
    "    else:\n",
    "        parts = folder_name.split()\n",
    "        return parts[-1].upper() if parts else folder_name.upper()\n",
    "\n",
    "def load_included_concepts(file_path):\n",
    "    \"\"\"Reads includedConcepts.csv and returns a clean DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --------------------------------------------\n",
    "# Group folders by disease (flat structure)\n",
    "# --------------------------------------------\n",
    "raw_folders = [f for f in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, f))]\n",
    "cleaned_folders = [strip_numeric_prefix(f) for f in raw_folders]\n",
    "\n",
    "disease_groups = defaultdict(list)\n",
    "for raw, clean in zip(raw_folders, cleaned_folders):\n",
    "    if re.search(r'ONLINE', clean, re.IGNORECASE):\n",
    "        continue\n",
    "    m = re.search(r'\\[(C\\d+)\\]', clean, re.IGNORECASE)\n",
    "    if m:\n",
    "        code = m.group(1).upper()\n",
    "        if code in TARGET_DISEASES:\n",
    "            disease_groups[code].append(raw)\n",
    "\n",
    "print(f\"\\nüß¨ Found {len(disease_groups)} target diseases.\")\n",
    "for k, v in sorted(disease_groups.items()):\n",
    "    print(f\"  {k} ({TARGET_DISEASES[k]}): {len(v)} arms\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Build concept matrix per disease\n",
    "# --------------------------------------------\n",
    "def build_concept_matrix(disease_code, folders):\n",
    "    disease_name = TARGET_DISEASES[disease_code]\n",
    "    print(f\"\\nüîç Processing {disease_code}_{disease_name}\")\n",
    "\n",
    "    arm_frames = []\n",
    "    arm_names = []\n",
    "    file_info = []\n",
    "\n",
    "    for f in folders:\n",
    "        fpath = os.path.join(ROOT_DIR, f, \"includedConcepts.csv\")\n",
    "        if not os.path.exists(fpath):\n",
    "            continue\n",
    "\n",
    "        arm_name = clean_arm_name(f)\n",
    "        if arm_name is None:\n",
    "            print(f\"   ‚è≠Ô∏è Skipping subgroup folder: {f}\")\n",
    "            continue\n",
    "\n",
    "        df = load_included_concepts(fpath)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df[\"Arm\"] = arm_name\n",
    "        arm_frames.append(df)\n",
    "        arm_names.append(arm_name)\n",
    "        file_info.append((arm_name, df.shape[0]))\n",
    "        print(f\"   üìÑ {arm_name:8s} ‚Üê {f}/includedConcepts.csv ({df.shape[0]} rows)\")\n",
    "\n",
    "    if not arm_frames:\n",
    "        print(f\"‚ö†Ô∏è No valid arms for {disease_code}_{disease_name}\")\n",
    "        return None\n",
    "\n",
    "    all_concepts = pd.concat(arm_frames, ignore_index=True)\n",
    "\n",
    "    # Find concept ID column\n",
    "    concept_col_candidates = [c for c in all_concepts.columns if re.search(r\"concept.?id\", c, re.IGNORECASE)]\n",
    "    if not concept_col_candidates:\n",
    "        print(f\"‚ö†Ô∏è No Concept ID column found for {disease_code}\")\n",
    "        return None\n",
    "    concept_col = concept_col_candidates[0]\n",
    "\n",
    "    # Pivot: rows = conceptId, cols = Arm\n",
    "    matrix = all_concepts.pivot_table(\n",
    "        index=concept_col,\n",
    "        columns=\"Arm\",\n",
    "        values=\"conceptSetId\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge back with concept metadata\n",
    "    meta_cols = [c for c in all_concepts.columns if c not in [\"Arm\", \"conceptSetId\"] + arm_names]\n",
    "    concept_meta = all_concepts.drop_duplicates(subset=[concept_col])[meta_cols]\n",
    "    merged = concept_meta.merge(matrix, on=concept_col, how=\"left\")\n",
    "\n",
    "    arm_cols = [c for c in merged.columns if c in arm_names]\n",
    "    merged[arm_cols] = (merged[arm_cols] > 0).astype(int)\n",
    "    merged.insert(0, \"Disease\", f\"{disease_code}_{disease_name}\")\n",
    "\n",
    "    # Save output\n",
    "    out_file = os.path.join(OUT_DIR, f\"{disease_code}_{disease_name}_arm_matrix.csv\")\n",
    "    merged.to_csv(out_file, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved: {out_file} ({merged.shape[0]} concepts, {len(arm_cols)} arms)\")\n",
    "    print(f\"   ‚îî‚îÄ Arms processed: {', '.join([f'{a} ({n})' for a, n in file_info])}\")\n",
    "    return merged\n",
    "\n",
    "# --------------------------------------------\n",
    "# Run for all diseases\n",
    "# --------------------------------------------\n",
    "for disease_code, folders in sorted(disease_groups.items()):\n",
    "    build_concept_matrix(disease_code, folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650dbb7-2e86-491a-8ef2-ba73fb0831f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(OUT_DIR, 'C04_DeepVeinThrombosis_arm_matrix.csv'), dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71aab2b-50c9-4d57-8317-15fd9793c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(OUT_DIR,'C06_Uveitis_arm_matrix.csv'),dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465ea4c-d8b5-40c5-ab98-567e7eb7b26c",
   "metadata": {},
   "source": [
    "### Phase A ‚Äî Team Blinding & Randomization (FRD 6.1.1‚Äì6.1.2)\n",
    "- This cell prepares **blinded arm matrices** for unbiased analysis by replacing original workflow labels with randomized labels like `SetA`, `SetB`, etc.  \n",
    "- It ensures traceability by also saving a **mapping key** linking original arm names to blinded labels.  \n",
    "- Inputs are `*_arm_matrix.csv` files; outputs include:\n",
    "  - Blinded matrices (`*_BLINDED_MATRIX.csv`) for statistical evaluation.  \n",
    "  - Mapping keys (`*_TEAM_MAPPING_KEY.csv`) for post-analysis reconciliation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227ff68-9841-4c81-9fbb-596de11226ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß© TEAM BLINDING & RANDOMIZATION SCRIPT (Refined)\n",
    "# ==========================================================\n",
    "# This script takes each *_arm_matrix.csv file,\n",
    "# randomizes arm labels (e.g., SetA, SetB, ...),\n",
    "# and creates a blinded dataset for unbiased analysis.\n",
    "# It also generates a mapping key for traceability.\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß≠ Path Configuration\n",
    "# --------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "INPUT_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_arm_concept_matrix\")\n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_blinded\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Optional reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# --------------------------------------------\n",
    "# üîπ Function: Get all disease arm-matrix CSV files\n",
    "# --------------------------------------------\n",
    "def get_disease_files(input_dir=INPUT_DIR, pattern=\"*_arm_matrix.csv\"):\n",
    "    \"\"\"\n",
    "    Finds all arm matrix CSV files inside the specified directory.\n",
    "    \"\"\"\n",
    "    search_path = os.path.join(input_dir, pattern)\n",
    "    return glob.glob(search_path)\n",
    "\n",
    "# --------------------------------------------\n",
    "# üîπ Function: Generate randomized Set labels\n",
    "# --------------------------------------------\n",
    "def generate_set_labels(n):\n",
    "    \"\"\"Generates labels like SetA, SetB, SetC... based on number of arms.\"\"\"\n",
    "    letters = string.ascii_uppercase\n",
    "    return [f\"Set{letters[i]}\" for i in range(n)]\n",
    "\n",
    "# --------------------------------------------\n",
    "# üîπ Function: Blind arm names randomly\n",
    "# --------------------------------------------\n",
    "def blind_arms_randomized(input_csv, output_csv, mapping_csv):\n",
    "    \"\"\"\n",
    "    Randomly replaces arm names with blinded labels (SetA, SetB, ...).\n",
    "    Saves both the blinded dataset and a mapping key for reference.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv, dtype=str)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Identify which columns represent arm flags (0/1)\n",
    "    # ----------------------------------------\n",
    "    ignore_cols = {\n",
    "        \"Disease\", \"conceptId\", \"conceptName\", \"conceptCode\",\n",
    "        \"conceptSetId\", \"domainId\", \"vocabularyId\",\n",
    "        \"standardConcept\", \"validStartDate\", \"validEndDate\",\n",
    "        \"concept_set_name\", \"conceptSetName\"\n",
    "    }\n",
    "\n",
    "    # Consider columns with binary-like values (0 or 1)\n",
    "    arm_cols = [\n",
    "        col for col in df.columns\n",
    "        if col not in ignore_cols\n",
    "        and set(df[col].dropna().unique()) <= {\"0\", \"1\"}\n",
    "    ]\n",
    "\n",
    "    if not arm_cols:\n",
    "        print(f\"‚ö†Ô∏è No valid arm columns found in: {input_csv}\")\n",
    "        return\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Create randomized blinding map\n",
    "    # ----------------------------------------\n",
    "    set_labels = generate_set_labels(len(arm_cols))\n",
    "    random.shuffle(set_labels)\n",
    "    randomized_mapping = dict(zip(arm_cols, set_labels))\n",
    "\n",
    "    # Apply mapping to dataset\n",
    "    blinded_df = df.rename(columns=randomized_mapping)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Save outputs\n",
    "    # ----------------------------------------\n",
    "    blinded_df.to_csv(output_csv, index=False)\n",
    "    pd.DataFrame(\n",
    "        list(randomized_mapping.items()),\n",
    "        columns=[\"Original_Arm\", \"Blinded_Label\"]\n",
    "    ).to_csv(mapping_csv, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Blinded matrix saved: {os.path.basename(output_csv)}\")\n",
    "    print(f\"üîë Mapping key saved: {os.path.basename(mapping_csv)}\")\n",
    "    print(f\"   ‚îî‚îÄ {len(arm_cols)} arms blinded ‚Üí {', '.join(set_labels)}\\n\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üîπ MAIN LOOP: Process all disease matrices\n",
    "# --------------------------------------------\n",
    "all_matrices = get_disease_files()\n",
    "\n",
    "if not all_matrices:\n",
    "    print(\"‚ö†Ô∏è No *_arm_matrix.csv files found in:\", INPUT_DIR)\n",
    "else:\n",
    "    print(f\"\\nüß© Found {len(all_matrices)} disease arm matrix files in {INPUT_DIR}\\n\")\n",
    "\n",
    "    for input_csv in sorted(all_matrices):\n",
    "        # Extract clean disease name\n",
    "        disease = os.path.splitext(os.path.basename(input_csv))[0]\n",
    "        disease = re.sub(r\"_arm_matrix$\", \"\", disease, flags=re.IGNORECASE)\n",
    "\n",
    "        # Define output filenames\n",
    "        blinded_output = os.path.join(OUTPUT_DIR, f\"{disease}_BLINDED_MATRIX.csv\")\n",
    "        mapping_output = os.path.join(OUTPUT_DIR, f\"{disease}_TEAM_MAPPING_KEY.csv\")\n",
    "\n",
    "        print(f\"üìÇ Processing: {os.path.basename(input_csv)}\")\n",
    "        blind_arms_randomized(input_csv, blinded_output, mapping_output)\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ All eligible disease matrices have been blinded.\")\n",
    "print(f\"üìÅ Blinded outputs saved in: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0a4c6-922f-4489-8a55-747a82e019a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(OUTPUT_DIR,'C07_SystemicSclerosis_BLINDED_MATRIX.csv'),dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2455cdb-10a9-45df-8a48-6f1618cc8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(OUTPUT_DIR,'C07_SystemicSclerosis_TEAM_MAPPING_KEY.csv'),dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217be6b3-1a74-40d5-9037-60364923385c",
   "metadata": {},
   "source": [
    "### Phase 1 ‚Äî Add Explicit Index to Blinded Matrices (FRD 6.1.1‚Äì6.1.4)\n",
    "- This cell processes all pre-generated blinded arm matrices (`*_BLINDED_MATRIX.csv`) by adding a **numeric `original_index` column** to uniquely track each concept.  \n",
    "- This ensures consistency when shuffling, merging, or analyzing concepts across workflows while preserving linkage to the original data.  \n",
    "- Updated files are saved as `*_FULL_WITH_INDEX.csv`, maintaining the full matrix structure for downstream **Phase A pre-adjudication analyses** and reproducibility.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76d613-ca1d-4c03-9e62-904e17c13c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üß© ADD EXPLICIT INDEX TO BLINDED MATRICES (Refined)\n",
    "# ===============================================================\n",
    "# Purpose:\n",
    "#   1Ô∏è‚É£ Load all blinded arm matrices (*_BLINDED_MATRIX.csv)\n",
    "#   2Ô∏è‚É£ Add a numeric \"original_index\" column (1-based)\n",
    "#   3Ô∏è‚É£ Save as *_FULL_WITH_INDEX.csv inside the same folder\n",
    "#\n",
    "# This ensures each concept retains a stable reference index\n",
    "# for downstream tracking and adjudication processes.\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß≠ Path Setup (relative to script)\n",
    "# --------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "BLINDED_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_blinded\")\n",
    "\n",
    "if not os.path.exists(BLINDED_DIR):\n",
    "    raise FileNotFoundError(f\"‚ö†Ô∏è Blinded directory not found: {BLINDED_DIR}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üßÆ Step 1: Load ConceptRecordCounts.csv (optional)\n",
    "# --------------------------------------------\n",
    "concept_count_file = os.path.join(SCRIPT_DIR, \"ConceptRecordCounts.csv\")\n",
    "if os.path.exists(concept_count_file):\n",
    "    record_count_df = pd.read_csv(concept_count_file, dtype=str)\n",
    "    print(f\"üìò Loaded ConceptRecordCounts.csv ({len(record_count_df):,} rows)\")\n",
    "else:\n",
    "    record_count_df = None\n",
    "    print(\"‚ö†Ô∏è ConceptRecordCounts.csv not found ‚Äî proceeding without it.\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üìÇ Step 2: Find all blinded matrix files\n",
    "# --------------------------------------------\n",
    "blinded_files = sorted(glob.glob(os.path.join(BLINDED_DIR, \"*_BLINDED_MATRIX.csv\")))\n",
    "\n",
    "if not blinded_files:\n",
    "    print(\"‚ö†Ô∏è No blinded matrix files found in:\", BLINDED_DIR)\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nüß© Found {len(blinded_files)} blinded matrix files to process:\\n\")\n",
    "for f in blinded_files:\n",
    "    print(f\"   ‚Üí {os.path.basename(f)}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß± Step 3: Add index and save updated copies\n",
    "# --------------------------------------------\n",
    "for file_path in blinded_files:\n",
    "    base_name = os.path.basename(file_path)\n",
    "    print(f\"\\nüìÇ Processing: {base_name}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read {base_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping {base_name}: file is empty.\")\n",
    "        continue\n",
    "\n",
    "    # Add 1-based index column\n",
    "    df.insert(0, \"original_index\", range(1, len(df) + 1))\n",
    "\n",
    "    # Build new output path\n",
    "    output_file = os.path.join(\n",
    "        BLINDED_DIR,\n",
    "        base_name.replace(\"_BLINDED_MATRIX.csv\", \"_FULL_WITH_INDEX.csv\")\n",
    "    )\n",
    "\n",
    "    # Save without auto index\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print summary stats\n",
    "    print(f\"‚úÖ Saved indexed file: {os.path.basename(output_file)}\")\n",
    "    print(f\"   Rows: {len(df):,} | Columns: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\nüéØ All blinded matrices updated with explicit 'original_index' column.\")\n",
    "print(f\"üìÅ Output directory: {BLINDED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ba1dd-dab7-431a-aee8-d45748a0f95a",
   "metadata": {},
   "source": [
    "### Phase 1 ‚Äî Randomize & Anonymize Blinded Matrices (FRD 5.1‚Äì5.2)\n",
    "- This cell processes blinded concept matrices to **anonymize and shuffle** them before adjudication, ensuring unbiased review.  \n",
    "- For each `_BLINDED_MATRIX.csv` file, it:\n",
    "  1. Adds an explicit original row index for traceability.  \n",
    "  2. Generates a **unique random 4‚Äì5 digit key** per row.  \n",
    "  3. Randomly shuffles rows and tracks which positions changed.  \n",
    "- The output files (`*_SHUFFLED.csv`) contain anonymized, traceable matrices ready for blinded adjudication, preserving both integrity and reproducibility.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b4b220-e597-4bfd-818d-4256346fe7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8424def-2ff2-45df-bc4b-b90622776a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üé≤ RANDOMIZE & ANONYMIZE BLINDED MATRICES (Refined)\n",
    "# ===============================================================\n",
    "# Purpose:\n",
    "#   1Ô∏è‚É£ Add explicit numeric index (\"original_index\")\n",
    "#   2Ô∏è‚É£ Generate unique random keys (4- or 5-digit)\n",
    "#   3Ô∏è‚É£ Shuffle rows for anonymization\n",
    "#   4Ô∏è‚É£ Validate uniqueness and save as *_SHUFFLED.csv\n",
    "#\n",
    "# Output:\n",
    "#   results/phaseA_shuffled/*.csv ‚Äî anonymized versions\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üß≠ PATH SETUP (relative to script)\n",
    "# ---------------------------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "INPUT_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_blinded\")\n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_shuffled\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîç STEP 1: Identify input blinded matrices\n",
    "# ---------------------------------------------------------------\n",
    "blinded_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*_BLINDED_MATRIX.csv\")))\n",
    "\n",
    "if not blinded_files:\n",
    "    print(f\"‚ö†Ô∏è No blinded matrix files found in: {INPUT_DIR}\")\n",
    "    raise SystemExit\n",
    "else:\n",
    "    print(f\"üß© Found {len(blinded_files)} blinded files to process in {INPUT_DIR}\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üéØ STEP 2: Process each file\n",
    "# ---------------------------------------------------------------\n",
    "for file_path in blinded_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"\\nüìÇ Processing file: {file_name}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping {file_name}: file is empty.\")\n",
    "        continue\n",
    "\n",
    "    # Ensure original_index exists or create it\n",
    "    if \"original_index\" not in df.columns:\n",
    "        df.insert(0, \"original_index\", range(1, len(df) + 1))\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üîë Generate random unique keys\n",
    "    # -----------------------------------------------------------\n",
    "    n = len(df)\n",
    "    np.random.seed(42)  # Reproducible randomization\n",
    "\n",
    "    # Dynamically scale key length\n",
    "    if n > 90000:\n",
    "        print(f\"‚ö†Ô∏è {file_name}: {n:,} rows ‚Äî using 6-digit keys.\")\n",
    "        all_possible = np.arange(100000, 999999)\n",
    "    elif n > 9000:\n",
    "        print(f\"‚ö†Ô∏è {file_name}: {n:,} rows ‚Äî using 5-digit keys.\")\n",
    "        all_possible = np.arange(10000, 99999)\n",
    "    else:\n",
    "        all_possible = np.arange(1000, 9999)\n",
    "\n",
    "    if n > len(all_possible):\n",
    "        raise ValueError(f\"‚ùå Dataset too large ({n:,} rows) for unique key generation.\")\n",
    "\n",
    "    random_keys = np.random.choice(all_possible, size=n, replace=False)\n",
    "    df.insert(1, \"key\", random_keys.astype(str))\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üîÄ Shuffle rows for anonymization\n",
    "    # -----------------------------------------------------------\n",
    "    shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    shuffled[\"index_changed\"] = shuffled[\"original_index\"].astype(int) != (shuffled.index + 1)\n",
    "    changed_count = shuffled[\"index_changed\"].sum()\n",
    "\n",
    "    print(f\"üîÑ Shuffled {changed_count:,} / {n:,} rows.\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # ‚úÖ Validate key uniqueness\n",
    "    # -----------------------------------------------------------\n",
    "    if shuffled[\"key\"].nunique() != n:\n",
    "        dup_keys = shuffled[shuffled[\"key\"].duplicated(keep=False)]\n",
    "        print(f\"‚ùå Duplicate keys found ({len(dup_keys)} rows):\")\n",
    "        print(dup_keys.head())\n",
    "        raise ValueError(f\"Duplicate keys detected in {file_name}\")\n",
    "\n",
    "    print(f\"‚úÖ All {n:,} keys are unique.\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üíæ Save anonymized shuffled version\n",
    "    # -----------------------------------------------------------\n",
    "    out_name = file_name.replace(\"_BLINDED_MATRIX.csv\", \"_SHUFFLED.csv\")\n",
    "    out_path = os.path.join(OUTPUT_DIR, out_name)\n",
    "    shuffled.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"üíæ Saved shuffled file: {out_path}\")\n",
    "    print(f\"   Rows: {len(shuffled):,} | Columns: {len(shuffled.columns)}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üßæ Summary\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\nüéØ All blinded matrices have been randomized and anonymized.\")\n",
    "print(f\"üìÅ Output folder: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29591107-1fdb-4355-aabd-041bf34d4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(OUTPUT_DIR,'C07_SystemicSclerosis_SHUFFLED.csv'),dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209a85a-8be4-427e-91fa-8daebf926270",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f40a9-186f-49e7-947d-c990dd03ed49",
   "metadata": {},
   "source": [
    "### Gold Standard Generation ‚Äî Blinded Matrix Processing (FRD 5.1, 6.2)\n",
    "- This cell processes the blinded, shuffled arm matrices to derive **two gold standard datasets per disease**:  \n",
    "  1. **Gold Standard 1 (GS1)**: Concepts with full agreement across all arms.  \n",
    "  2. **Gold Standard 2 (GS2)**: Concepts with partial agreement (some but not all arms).  \n",
    "- Both outputs are annotated with record counts (if available) and priority flags (`high`/`low`) and are saved as CSV files for downstream review, adjudication, and comparison with AI and Human workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082519c5-ac40-4158-a4b8-32682289cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üß¨ GOLD STANDARD GENERATION ‚Äî SCHEMA-PRESERVING VERSION\n",
    "# ===============================================================\n",
    "# Purpose:\n",
    "#   1Ô∏è‚É£ Load each *_SHUFFLED.csv file (blinded + randomized)\n",
    "#   2Ô∏è‚É£ Identify consensus across all Set columns (SetA, SetB, etc.)\n",
    "#   3Ô∏è‚É£ Generate:\n",
    "#        - Gold Standard 1 ‚Üí full agreement (all Sets = 1)\n",
    "#        - Gold Standard 2 ‚Üí partial agreement (1‚Äìmax-1 Sets = 1)\n",
    "#   4Ô∏è‚É£ Retain *all* original columns\n",
    "#   5Ô∏è‚É£ Merge record counts and priority metadata at the end\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß≠ PATH CONFIGURATION\n",
    "# --------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "INPUT_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_shuffled\")\n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_gold\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------------------------------------------\n",
    "# üìò LOAD RECORD COUNT REFERENCE (if exists)\n",
    "# --------------------------------------------\n",
    "record_count_path = os.path.join(SCRIPT_DIR, \"ConceptRecordCounts.csv\")\n",
    "if os.path.exists(record_count_path):\n",
    "    record_count_df = pd.read_csv(record_count_path, dtype=str)\n",
    "    record_count_df[\"record_count\"] = pd.to_numeric(record_count_df[\"record_count\"], errors=\"coerce\")\n",
    "    print(f\"üìò Loaded ConceptRecordCounts.csv ({len(record_count_df):,} rows)\")\n",
    "else:\n",
    "    record_count_df = None\n",
    "    print(\"‚ö†Ô∏è ConceptRecordCounts.csv not found ‚Äî record counts will be skipped.\")\n",
    "\n",
    "# ===============================================================\n",
    "# üß© FUNCTION: Process One Shuffled Matrix\n",
    "# ===============================================================\n",
    "def process_blinded_matrix(filepath, record_count_df):\n",
    "    \"\"\"\n",
    "    Processes one *_SHUFFLED.csv file to derive:\n",
    "      - Gold Standard 1 (all Sets = 1)\n",
    "      - Gold Standard 2 (1‚Äìmax-1 Sets = 1)\n",
    "    Retains all original columns and appends derived metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(filepath, dtype=str)\n",
    "    df.fillna(\"0\", inplace=True)\n",
    "    disease_name = os.path.splitext(os.path.basename(filepath))[0].replace(\"_SHUFFLED\", \"\")\n",
    "\n",
    "    print(f\"\\nüìÇ Processing: {disease_name}\")\n",
    "    print(f\"   ‚Üí File: {os.path.basename(filepath)} | Rows: {len(df):,}\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üîç Identify Set columns dynamically\n",
    "    # -----------------------------------------------------------\n",
    "    set_cols = [c for c in df.columns if re.match(r\"^Set[A-Z]+$\", c)]\n",
    "    if not set_cols:\n",
    "        print(f\"‚ö†Ô∏è No Set columns found in {disease_name}. Skipping.\")\n",
    "        return None, None\n",
    "\n",
    "    # Convert to numeric (binary 0/1)\n",
    "    df[set_cols] = df[set_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    max_sum = len(set_cols)\n",
    "    df[\"sum_sets\"] = df[set_cols].sum(axis=1)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üß© Gold Standard 1 ‚Äî Full Agreement (all Sets = 1)\n",
    "    # -----------------------------------------------------------\n",
    "    df_gs1 = df[df[\"sum_sets\"] == max_sum].copy()\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üß© Gold Standard 2 ‚Äî Partial Agreement (1‚Äìmax-1)\n",
    "    # -----------------------------------------------------------\n",
    "    df_gs2 = df[df[\"sum_sets\"].between(1, max_sum - 1)].copy()\n",
    "    df_gs2[\"priority\"] = df_gs2[\"sum_sets\"].apply(lambda x: \"high\" if x <= 2 else \"low\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üîó Merge record counts (if available)\n",
    "    # -----------------------------------------------------------\n",
    "    if record_count_df is not None and \"conceptId\" in df.columns:\n",
    "        df_gs2 = df_gs2.merge(\n",
    "            record_count_df[[\"conceptId\", \"record_count\"]],\n",
    "            on=\"conceptId\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üß± Preserve all original columns\n",
    "    # -----------------------------------------------------------\n",
    "    original_cols = list(df.columns)\n",
    "    extra_cols_gs1 = [\"sum_sets\"]\n",
    "    extra_cols_gs2 = [\"sum_sets\", \"record_count\", \"priority\"]\n",
    "\n",
    "    # Ensure no duplicates and maintain order\n",
    "    gs1_cols = [c for c in original_cols if c not in extra_cols_gs1] + extra_cols_gs1\n",
    "    gs2_cols = [c for c in original_cols if c not in extra_cols_gs2] + extra_cols_gs2\n",
    "\n",
    "    # Add any missing columns (if not in data)\n",
    "    for col in gs1_cols:\n",
    "        if col not in df_gs1.columns:\n",
    "            df_gs1[col] = \"\"\n",
    "    for col in gs2_cols:\n",
    "        if col not in df_gs2.columns:\n",
    "            df_gs2[col] = \"\"\n",
    "\n",
    "    df_gs1 = df_gs1.reindex(columns=gs1_cols)\n",
    "    df_gs2 = df_gs2.reindex(columns=gs2_cols)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üíæ Save outputs\n",
    "    # -----------------------------------------------------------\n",
    "    gs1_out = os.path.join(OUTPUT_DIR, f\"{disease_name}_Gold_Standard_1.csv\")\n",
    "    gs2_out = os.path.join(OUTPUT_DIR, f\"{disease_name}_Gold_Standard_2.csv\")\n",
    "\n",
    "    df_gs1.to_csv(gs1_out, index=False)\n",
    "    df_gs2.to_csv(gs2_out, index=False)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üìä Summary\n",
    "    # -----------------------------------------------------------\n",
    "    print(f\"‚úÖ Gold Standards saved for {disease_name}\")\n",
    "    print(f\"   ‚îú‚îÄ GS1 (Full Agreement): {len(df_gs1):,} rows\")\n",
    "    print(f\"   ‚îî‚îÄ GS2 (Partial Agreement): {len(df_gs2):,} rows\")\n",
    "    print(f\"   Columns preserved: {len(original_cols)} original + derived\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    return df_gs1, df_gs2\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# üöÄ MAIN EXECUTION\n",
    "# ===============================================================\n",
    "shuffled_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*_SHUFFLED.csv\")))\n",
    "\n",
    "if not shuffled_files:\n",
    "    print(f\"‚ö†Ô∏è No shuffled files found in {INPUT_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nüß© Found {len(shuffled_files)} shuffled files to process in {INPUT_DIR}\")\n",
    "\n",
    "    for file in shuffled_files:\n",
    "        process_blinded_matrix(file, record_count_df)\n",
    "\n",
    "print(\"\\nüéØ Gold Standard generation complete.\")\n",
    "print(f\"üìÅ Outputs saved in: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a66c7-5db1-4b70-8ca0-b3597a03c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(OUTPUT_DIR,'C03_DiabetesMacularEdema_Gold_Standard_2.csv'),dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f5121-b703-4f51-be1b-a273d475286e",
   "metadata": {},
   "source": [
    "### Adjudication File Preparation ‚Äî Clean Gold Standard 2 (FRD 5 & 6.2)\n",
    "- This cell processes all `*_Gold_Standard_2.csv` files to generate **minimal, human-readable adjudication datasets**.  \n",
    "- It removes internal identifiers, blinded team labels, metadata columns, and adds blank columns (`keepConceptSet`, `comment`) for adjudicators to record decisions.  \n",
    "- Cleaned files are saved in the `Adjudication/` folder as `<disease>_for_adjudication.csv`, ready for blinded human review and TGS finalization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c5214e-da4e-43f4-91a1-7cc4d6c2f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üßæ ADJUDICATION FILE PREPARATION ‚Äî CLEANING GOLD STANDARD 2 (Refined)\n",
    "# ===============================================================\n",
    "# Purpose:\n",
    "#   1Ô∏è‚É£ Load each *_Gold_Standard_2.csv file\n",
    "#   2Ô∏è‚É£ Remove blinded/internal columns\n",
    "#   3Ô∏è‚É£ Sort back to original order (if available)\n",
    "#   4Ô∏è‚É£ Add blank adjudication columns\n",
    "#   5Ô∏è‚É£ Save as *_for_adjudication.csv in output folder\n",
    "#\n",
    "# Result:\n",
    "#   A clean, minimal dataset ready for human adjudication.\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üß≠ PATH CONFIGURATION\n",
    "# ---------------------------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "INPUT_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_gold\")\n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_adjudication\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Input directory : {INPUT_DIR}\")\n",
    "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üßπ STEP 1: Define columns to remove before adjudication\n",
    "# ---------------------------------------------------------------\n",
    "base_remove_cols = {\n",
    "    \"original_index\", \"index_changed\", \"sum_sets\",\n",
    "    \"record_count\", \"priority\"\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üß© STEP 2: Locate all Gold Standard 2 files\n",
    "# ---------------------------------------------------------------\n",
    "gold2_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"*_Gold_Standard_2.csv\")))\n",
    "\n",
    "if not gold2_files:\n",
    "    print(\"‚ö†Ô∏è No Gold Standard 2 files found in:\", INPUT_DIR)\n",
    "    raise SystemExit\n",
    "else:\n",
    "    print(f\"üß© Found {len(gold2_files)} Gold Standard 2 files to process.\\n\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üöÄ STEP 3: Process each file\n",
    "# ---------------------------------------------------------------\n",
    "for file_path in gold2_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"\\nüìÇ Cleaning file: {file_name}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to read {file_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è Skipping {file_name}: file is empty.\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üß≠ Sort back to original order if tracked\n",
    "    # -----------------------------------------------------------\n",
    "    if \"original_index\" in df.columns:\n",
    "        df[\"original_index\"] = pd.to_numeric(df[\"original_index\"], errors=\"coerce\")\n",
    "        df = df.sort_values(by=\"original_index\", kind=\"stable\").reset_index(drop=True)\n",
    "        print(\"‚Ü©Ô∏è Sorted by 'original_index' to restore original order.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No 'original_index' column found ‚Äî order preserved as-is.\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üß© Identify Set columns dynamically (blinded arms)\n",
    "    # -----------------------------------------------------------\n",
    "    set_cols = [c for c in df.columns if re.match(r\"^Set[A-Z]+$\", c)]\n",
    "    remove_cols = base_remove_cols.union(set_cols)\n",
    "\n",
    "    # Drop internal/blinded columns\n",
    "    df_clean = df.drop(columns=remove_cols, errors=\"ignore\").reset_index(drop=True)\n",
    "    print(f\"üßº Dropped {len(remove_cols)} internal/blinded columns.\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # ‚ú® Add adjudication columns (if not present)\n",
    "    # -----------------------------------------------------------\n",
    "    for col in [\"keepConceptSet\", \"comment\"]:\n",
    "        if col not in df_clean.columns:\n",
    "            df_clean[col] = \"\"\n",
    "\n",
    "    # Ensure new columns appear at the end\n",
    "    ordered_cols = [c for c in df_clean.columns if c not in [\"keepConceptSet\", \"comment\"]] + [\"keepConceptSet\", \"comment\"]\n",
    "    df_clean = df_clean[ordered_cols]\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üíæ Save cleaned file for adjudication\n",
    "    # -----------------------------------------------------------\n",
    "    disease_name = file_name.replace(\"_Gold_Standard_2.csv\", \"\")\n",
    "    adjud_out = os.path.join(OUTPUT_DIR, f\"{disease_name}_for_adjudication.csv\")\n",
    "    df_clean.to_csv(adjud_out, index=False)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # üìä Summary report\n",
    "    # -----------------------------------------------------------\n",
    "    print(f\"‚úÖ Saved adjudication file: {os.path.basename(adjud_out)}\")\n",
    "    print(f\"   Rows: {df_clean.shape[0]:,} | Columns: {df_clean.shape[1]}\")\n",
    "    print(f\"   ‚Üí Preview columns: {', '.join(df_clean.columns[:6])} ...\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\nüéØ All Gold Standard 2 files cleaned and ready for adjudication.\")\n",
    "print(f\"üìÅ Output folder: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509622d7-0150-4afe-89cd-7a01caa540b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(OUTPUT_DIR,'C04_DeepVeinThrombosis_for_adjudication.csv'),dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec637f48-19db-4596-ba25-9fe7faf8d2c6",
   "metadata": {},
   "source": [
    "### Adjudication ‚Äî Summary of Concepts Submitted for Review (FRD 5.1, 6.2)\n",
    "- This cell counts the **concept sets submitted for adjudication** for each disease before gold standard resolution.  \n",
    "- For each disease, it compares the number of concepts in AI/Human submissions with the counts in **Gold Standard files** (`Gold_Standard_1.csv` and `Gold_Standard_2.csv`) to quantify concepts requiring adjudication.  \n",
    "- The output **`adjudication_summary.csv`** provides a table with:\n",
    "  - Total concepts per disease, Gold Standard 1 count, and the number of concepts going for adjudication.  \n",
    "  - Supports evaluation of disagreement magnitude and guides Phase B post-adjudication analyses.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b7dd4-ca49-4862-bbb2-fda428d6fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üìä SUMMARY ‚Äî CONCEPT SETS SENT FOR ADJUDICATION (Refined)\n",
    "# ===============================================================\n",
    "# Purpose:\n",
    "#   1Ô∏è‚É£ Reads all *_for_adjudication.csv files\n",
    "#   2Ô∏è‚É£ Looks up corresponding Gold Standard 1 & 2 files\n",
    "#   3Ô∏è‚É£ Computes totals and adjudication proportions\n",
    "#   4Ô∏è‚É£ Saves and prints summary table\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üß≠ PATH CONFIGURATION\n",
    "# ---------------------------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "ADJUD_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_adjudication\")\n",
    "GOLD_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_gold\")\n",
    "SUMMARY_PATH = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_adjudication_summary.csv\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîç Locate all adjudication files\n",
    "# ---------------------------------------------------------------\n",
    "adjudication_files = sorted(glob.glob(os.path.join(ADJUD_DIR, \"*_for_adjudication.csv\")))\n",
    "\n",
    "if not adjudication_files:\n",
    "    print(f\"‚ö†Ô∏è No adjudication files found in {ADJUD_DIR}\")\n",
    "    raise SystemExit\n",
    "\n",
    "# print(f\"üß© Found {len(adjudication_files)} adjudication files to summarize.\\n\")\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üß© Process each adjudication file\n",
    "# ---------------------------------------------------------------\n",
    "for file_path in adjudication_files:\n",
    "    disease_name = os.path.basename(file_path).replace(\"_for_adjudication.csv\", \"\")\n",
    "    # print(f\"üìÇ {disease_name}\")\n",
    "\n",
    "    # Load adjudication file\n",
    "    try:\n",
    "        df_adj = pd.read_csv(file_path, dtype=str)\n",
    "        adjud_count = len(df_adj)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Corresponding GS1 / GS2 paths\n",
    "    gs1_path = os.path.join(GOLD_DIR, f\"{disease_name}_Gold_Standard_1.csv\")\n",
    "    gs2_path = os.path.join(GOLD_DIR, f\"{disease_name}_Gold_Standard_2.csv\")\n",
    "\n",
    "    gs1_count = 0\n",
    "    gs2_count = 0\n",
    "\n",
    "    if os.path.exists(gs1_path):\n",
    "        gs1_count = len(pd.read_csv(gs1_path, dtype=str))\n",
    "    if os.path.exists(gs2_path):\n",
    "        gs2_count = len(pd.read_csv(gs2_path, dtype=str))\n",
    "\n",
    "    total_concepts = gs1_count + gs2_count\n",
    "\n",
    "    # Compute adjudication %\n",
    "    adjud_percent = (adjud_count / total_concepts * 100) if total_concepts > 0 else 0\n",
    "\n",
    "    summary_data.append({\n",
    "        \"Disease\": disease_name,\n",
    "        \"Total_Concepts\": total_concepts,\n",
    "        \"Gold_Standard_1_Count\": gs1_count,\n",
    "        \"Gold_Standard_2_Count\": gs2_count,\n",
    "        \"Concepts_for_Adjudication\": adjud_count,\n",
    "        \"Adjudication_%\": round(adjud_percent, 2)\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üìä Create summary DataFrame\n",
    "# ---------------------------------------------------------------\n",
    "summary_df = pd.DataFrame(summary_data).sort_values(by=\"Concepts_for_Adjudication\", ascending=False)\n",
    "\n",
    "# Save to CSV\n",
    "os.makedirs(os.path.dirname(SUMMARY_PATH), exist_ok=True)\n",
    "summary_df.to_csv(SUMMARY_PATH, index=False)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üñ®Ô∏è Display nicely\n",
    "# ---------------------------------------------------------------\n",
    "# print(\"\\n‚úÖ Adjudication Summary Table\\n\")\n",
    "# print(summary_df.to_string(index=False))\n",
    "# # print(f\"\\nüíæ Saved summary table ‚Üí {SUMMARY_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ca42ea-7b25-4c96-9a21-cceb7e0262ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a7152-1c50-4774-97e5-f3aac10b6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('Adjudication/C03_DiabetesMacularEdema_for_adjudication.csv',dtype=str)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad607c-fd83-415a-a2dc-c7288c9af707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
