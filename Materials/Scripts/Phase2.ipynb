{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase B ‚Äî Gold Standard Finalization and Adjudication Merge (FRD 5, 6.2.1)\n",
    "- This cell processes **post-adjudication** outputs to generate the finalized Gold Standard for each disease.  \n",
    "- It reads `_for_adjudication.csv` files, filters concepts marked `\"YES\"` to keep, aligns and cleans **Gold Standard 1 & 2** datasets, and preserves a consistent metadata + set column order.  \n",
    "- The merged and cleaned outputs are saved as `<disease>_Gold_Standard_FINAL.csv`, which will be used for **accuracy evaluation, weighted/unweighted F1 calculations, and downstream Phase B analyses**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 0: Path setup (robust)\n",
    "# --------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()  # fallback for notebooks\n",
    "\n",
    "ADJUD_DIR = os.path.join(SCRIPT_DIR, '..', 'ClinicalAdjudicationFiles')\n",
    "os.makedirs(os.path.join(SCRIPT_DIR, 'results'), exist_ok=True)\n",
    "\n",
    "# Define the correct desired column order\n",
    "metadata_cols = [\n",
    "    \"original_index\", \"key\", \"conceptSetId\", \"conceptId\", \"conceptName\",\n",
    "    \"standardConcept\", \"invalidReason\", \"conceptCode\", \"vocabularyId\"\n",
    "]\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 1: Find all adjudication Excel files recursively\n",
    "# --------------------------------------------\n",
    "adjud_files = glob.glob(os.path.join(ADJUD_DIR, \"**\", \"*_for_adjudication.xlsx\"), recursive=True)\n",
    "\n",
    "\n",
    "if not adjud_files:\n",
    "    print(\"‚ö†Ô∏è No adjudication Excel files found.\")\n",
    "else:\n",
    "    print(f\"üß© Found {len(adjud_files)} adjudication Excel files.\\n\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 2: Convert .xlsx ‚Üí .csv (if not already converted)\n",
    "# --------------------------------------------\n",
    "converted_files = []\n",
    "for adj_path in adjud_files:\n",
    "    disease_prefix = os.path.splitext(os.path.basename(adj_path))[0].replace(\"_for_adjudication\", \"\")\n",
    "    csv_path = os.path.join(os.path.dirname(adj_path), f\"{disease_prefix}_for_adjudication.csv\")\n",
    "\n",
    "    # Only convert if CSV doesn't already exist or Excel is newer\n",
    "    if not os.path.exists(csv_path) or os.path.getmtime(adj_path) > os.path.getmtime(csv_path):\n",
    "        try:\n",
    "            df = pd.read_excel(adj_path, dtype=str)\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"‚úÖ Converted: {os.path.basename(adj_path)} ‚Üí {os.path.basename(csv_path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to convert {adj_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    converted_files.append(csv_path)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 3: Use converted CSVs for processing\n",
    "# --------------------------------------------\n",
    "adjud_files = converted_files\n",
    "# print(f\"üóÇÔ∏è Proceeding with {len(adjud_files)} CSV adjudication files.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üß¨ FINAL GOLD STANDARD GENERATION ‚Äî WITH CORRECT PATHS\n",
    "# ===============================================================\n",
    "# Purpose:\n",
    "#   1Ô∏è‚É£ Load each adjudicated *_for_adjudication.csv file (from ClinicalAdjudicationFiles)\n",
    "#   2Ô∏è‚É£ Load corresponding Gold Standard 1 (from phaseA_gold)\n",
    "#   3Ô∏è‚É£ Keep only adjudicated ‚ÄúYES‚Äù concepts from GS2\n",
    "#   4Ô∏è‚É£ Merge GS1 + adjudicated GS2 ‚Üí FINAL Gold Standard\n",
    "#   5Ô∏è‚É£ Save to results/phaseA_gold_final/\n",
    "#   6Ô∏è‚É£ Create a summary report\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üß≠ PATH CONFIGURATION\n",
    "# ---------------------------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "# Phase A Gold (GS1)\n",
    "GOLD_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_gold\")\n",
    "\n",
    "# Adjudicated GS2 (from clinical adjudication folders)\n",
    "ADJUD_DIR = os.path.join(SCRIPT_DIR, \"..\", \"ClinicalAdjudicationFiles\")\n",
    "\n",
    "# Output folder for final merged GS\n",
    "FINAL_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_gold_final\")\n",
    "os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "\n",
    "# Preserve standard metadata order\n",
    "metadata_cols = [\n",
    "    \"original_index\", \"key\", \"Disease\", \"conceptSetId\", \"conceptId\", \"conceptName\",\n",
    "    \"standardConcept\", \"invalidReason\", \"conceptCode\", \"vocabularyId\"\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîç Locate all adjudicated GS2 CSVs\n",
    "# ---------------------------------------------------------------\n",
    "adjud_files = sorted(glob.glob(os.path.join(ADJUD_DIR, \"**\", \"*_for_adjudication.csv\"), recursive=True))\n",
    "\n",
    "if not adjud_files:\n",
    "    print(f\"‚ö†Ô∏è No adjudicated GS2 files found in {ADJUD_DIR}\")\n",
    "    raise SystemExit\n",
    "\n",
    "# print(f\"üß© Found {len(adjud_files)} adjudicated GS2 files to merge.\\n\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üß© Process each adjudicated GS2 file\n",
    "# ---------------------------------------------------------------\n",
    "for adj_file in adjud_files:\n",
    "    disease_prefix = os.path.basename(adj_file).replace(\"_for_adjudication.csv\", \"\")\n",
    "    gs1_path = os.path.join(GOLD_DIR, f\"{disease_prefix}_Gold_Standard_1.csv\")\n",
    "    final_path = os.path.join(FINAL_DIR, f\"{disease_prefix}_Gold_Standard_FINAL.csv\")\n",
    "\n",
    "    if not os.path.exists(gs1_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping {disease_prefix}: Missing GS1 file ‚Üí {gs1_path}\")\n",
    "        continue\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 1: Load adjudicated GS2\n",
    "    # -----------------------------------------------------------\n",
    "    try:\n",
    "        adj_df = pd.read_csv(adj_file, dtype=str)\n",
    "        adj_df.columns = adj_df.columns.str.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {adj_file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"key\" not in adj_df.columns or \"keepConceptSet\" not in adj_df.columns:\n",
    "        print(f\"‚ö†Ô∏è Skipping {disease_prefix}: Missing 'key' or 'keepConceptSet' column.\")\n",
    "        continue\n",
    "\n",
    "    total_before = len(adj_df)\n",
    "    adj_keep = adj_df[adj_df[\"keepConceptSet\"].str.strip().str.upper() == \"YES\"].copy()\n",
    "    total_after = len(adj_keep)\n",
    "\n",
    "    print(f\"=== Disease: {disease_prefix} ===\")\n",
    "    # print(f\"   ‚Üí Adjudicated YES: {total_after:,} / {total_before:,}\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 2: Load GS1 (always keep all)\n",
    "    # -----------------------------------------------------------\n",
    "    gs1_df = pd.read_csv(gs1_path, dtype=str)\n",
    "    gs1_df = gs1_df.drop(columns=[\"index_changed\", \"sum_sets\"], errors=\"ignore\")\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 3: Align columns dynamically\n",
    "    # -----------------------------------------------------------\n",
    "    all_cols = metadata_cols.copy()\n",
    "    set_cols = sorted([c for c in gs1_df.columns if c.startswith(\"Set\") or c.startswith(\"SET\")])\n",
    "    ordered_cols = all_cols + set_cols\n",
    "\n",
    "    # Reindex adjudicated GS2\n",
    "    adj_df = adj_df.reindex(columns=ordered_cols, fill_value=\"\")\n",
    "    gs1_df = gs1_df.reindex(columns=ordered_cols, fill_value=\"\")\n",
    "\n",
    "    # Keep only adjudicated YES rows in GS2\n",
    "    if \"key\" in adj_df.columns:\n",
    "        adj_filtered = adj_df[adj_df[\"key\"].isin(adj_keep[\"key\"])].copy()\n",
    "    else:\n",
    "        adj_filtered = pd.DataFrame(columns=ordered_cols)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # STEP 4: Merge GS1 + adjudicated GS2\n",
    "    # -----------------------------------------------------------\n",
    "    final_df = pd.concat([gs1_df, adj_filtered], ignore_index=True)\n",
    "    final_df.to_csv(final_path, index=False)\n",
    "\n",
    "    # print(f\"   ‚úÖ Saved FINAL Gold Standard ‚Üí {os.path.basename(final_path)}\")\n",
    "    # print(f\"   ‚îú‚îÄ GS1 rows kept: {len(gs1_df):,}\")\n",
    "    # print(f\"   ‚îú‚îÄ Adjudicated GS2 rows kept: {len(adj_filtered):,}\")\n",
    "    print(f\"    The Gold Standard concepts: {len(final_df):,}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"Disease\": disease_prefix,\n",
    "        \"GS1_Rows\": len(gs1_df),\n",
    "        \"GS2_Adjudicated_Rows\": len(adj_filtered),\n",
    "        \"Final_Total\": len(final_df)\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üìä Step 5: Save Summary Table\n",
    "# ---------------------------------------------------------------\n",
    "if summary_rows:\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_path = os.path.join(FINAL_DIR, \"Final_Gold_Standard_Summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    # print(\"\\nüìà Final Gold Standard Merge Summary\\n\")\n",
    "    # print(summary_df.to_string(index=False))\n",
    "    # print(f\"\\nüíæ Saved summary table ‚Üí {summary_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No final gold standards created ‚Äî please verify adjudicated GS2 files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Phase B ‚Äî Gold Standard QA: Duplicate and Missing Key Check (FRD 6.2.1)\n",
    "- This cell performs a **quality assurance check** on the post-adjudication datasets before final Gold Standard generation.  \n",
    "- It identifies duplicate keys in `GS2`, compares the keys marked `\"YES\"` in adjudication with `GS2`, and reports missing or extra keys.  \n",
    "- These checks ensure that all approved concepts are captured in the Gold Standard and highlight potential **data inconsistencies** for review prior to downstream accuracy calculations.   -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for duplicate keys in GS2 #DEBUG\n",
    "# dup_keys = gs1_df[\"key\"].value_counts()\n",
    "# dup_keys = dup_keys[dup_keys > 1]\n",
    "# if not dup_keys.empty:\n",
    "#     print(f\"‚ö†Ô∏è Found {len(dup_keys)} duplicate keys in {gs2_file}\")\n",
    "#     print(dup_keys.head(10))\n",
    "\n",
    "# # Compare key sets directly\n",
    "# missing_in_gs2 = keep_keys - set(gs2_df[\"key\"])\n",
    "# extra_in_gs2 = set(gs2_df[\"key\"]) - keep_keys\n",
    "\n",
    "# print(f\"üßæ Keys in adjudication (YES): {len(keep_keys)}\")\n",
    "# print(f\"üßæ Keys in GS2: {len(gs2_df['key'].unique())}\")\n",
    "# print(f\"‚ùå Keys in adjudication but not in GS2: {len(missing_in_gs2)}\")\n",
    "# print(f\"‚ö†Ô∏è Duplicated keys in GS2: {len(dup_keys)}\")\n",
    "\n",
    "# if missing_in_gs2:\n",
    "#     print(\"\\nüîç Missing keys (in adjudication but not in GS2):\")\n",
    "#     print(list(missing_in_gs2)[:20])  # show first 20 only\n",
    "#     print(f\"... and {len(missing_in_gs2) - 20} more\" if len(missing_in_gs2) > 20 else \"\")\n",
    "# else:\n",
    "#     print(\"\\n‚úÖ No missing keys ‚Äî all adjudicated YES keys found in GS2.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('C02_DiabetesMelltius_Gold_Standard_FINAL.csv',dtype=str)\n",
    "# df.columns #DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('C01_ObsessiveCompulsiveDisorder_Gold_Standard_FINAL.csv',dtype=str)\n",
    "# df.head() #DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase B ‚Äî Arm vs Final Gold Standard Comparison (FRD 6.2.1, 6.2.2)\n",
    "- This cell compares each original arm‚Äôs concept set (Human or AI) to the **final adjudicated Gold Standard** for each disease.  \n",
    "- It computes counts of concepts overlapping with, unique to the arm, and unique to the final set, as well as the overlap percentage.  \n",
    "- Outputs are saved in `gold_standard_comparison_summary.csv` for downstream **accuracy metrics, F1 calculations, and clinical impact analyses**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ###  Phase B ‚Äî Primary Objective Analysis (Weighted F1 vs TGS) (FRD 6.2.1)\n",
    "\n",
    "1. **Compare Arm Outputs to Gold Standard (FRD 6.2.1):** This cell loads all Human and AI workflow concept sets for each disease and compares them to the adjudicated True Gold Standard (TGS), calculating both weighted and unweighted precision, recall, and F1 scores.  \n",
    "2. **Weighted Metrics Using Concept Prevalence (FRD 6.2.1):** Weighted counts of True Positives (WTP), False Positives (WFP), and False Negatives (WFN) are computed using `ConceptRecordCounts.csv` to generate prevalence-weighted F1 scores for each workflow.  \n",
    "3. **Output Saved (CSV):** Results are stored in `gold_standard_comparison_summary.csv`, summarizing per-disease metrics for all arms, including overlaps, Jaccard similarity, weighted/unweighted precision, recall, and F1 scores.\n",
    "\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üß¨ VALIDATION ‚Äî COMPARE ORIGINAL ARMS VS FINAL GOLD STANDARD\n",
    "# ===============================================================\n",
    "# Purpose:\n",
    "#   1Ô∏è‚É£ Detect correct arm folders inside ConceptSets/\n",
    "#   2Ô∏è‚É£ Fallback to phaseA_pre_adjudication summaries if needed\n",
    "#   3Ô∏è‚É£ Compare all arm concepts vs adjudicated gold standard\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üß≠ PATH CONFIGURATION\n",
    "# ---------------------------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "CONCEPTSET_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, \"..\", \"ConceptSets\"))\n",
    "PHASEA_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_pre_adjudication\")\n",
    "FINAL_GOLD_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_gold_final\")\n",
    "OUTPUT_PATH = os.path.join(SCRIPT_DIR, \"results\", \"gold_standard_comparison_summary.csv\")\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ Helper: Clean Arm Names\n",
    "# ---------------------------------------------------------------\n",
    "def clean_arm_name(folder_name):\n",
    "    if re.search(r\"\\[S\\d+\\]\", folder_name, re.IGNORECASE):\n",
    "        return None\n",
    "    match = re.search(r\"\\[(AI\\d+|H\\d+|HUMAN)\\]\", folder_name, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    if \"AI\" in folder_name.upper():\n",
    "        m = re.search(r\"AI\\d+\", folder_name.upper())\n",
    "        if m:\n",
    "            return m.group(0)\n",
    "    if \"H1\" in folder_name.upper() or \"HUMAN\" in folder_name.upper():\n",
    "        return \"H1\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ Helper: Load concept IDs from includedConcepts.csv\n",
    "# ---------------------------------------------------------------\n",
    "def load_included_concepts(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        df.columns = [c.strip() for c in df.columns]\n",
    "        norm_map = {re.sub(r'[^a-z0-9]', '', c.lower()): c for c in df.columns}\n",
    "        chosen = norm_map.get(\"conceptid\") or next(\n",
    "            (orig for norm, orig in norm_map.items()\n",
    "             if \"concept\" in norm and \"id\" in norm and \"set\" not in norm),\n",
    "            None\n",
    "        )\n",
    "        if not chosen:\n",
    "            return set()\n",
    "        vals = df[chosen].dropna().astype(str).str.strip().str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "        return set(vals[vals != \"\"])\n",
    "    except Exception:\n",
    "        return set()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ Analyze disease using ConceptSets (preferred)\n",
    "# ---------------------------------------------------------------\n",
    "def analyze_disease_conceptsets(disease_name, final_file):\n",
    "    \"\"\"\n",
    "    Analyze overlap between ConceptSets and Final Gold Standard (flat structure).\n",
    "    Looks for [Cxx][AIx]/[H1] folders directly under ConceptSets/.\n",
    "    \"\"\"\n",
    "    disease_prefix = re.match(r\"(C\\d+)\", disease_name)\n",
    "    if not disease_prefix:\n",
    "        print(f\"‚ö†Ô∏è Could not parse prefix for {disease_name}\")\n",
    "        return None\n",
    "    prefix = disease_prefix.group(1)\n",
    "\n",
    "    # üîç Find all concept set folders matching the prefix (e.g. [C01])\n",
    "    candidate_folders = [\n",
    "        f for f in os.listdir(CONCEPTSET_DIR)\n",
    "        if re.search(rf\"\\[{prefix}\\]\", f, re.IGNORECASE)\n",
    "    ]\n",
    "\n",
    "    if not candidate_folders:\n",
    "        print(f\"‚ö†Ô∏è No folders found for {disease_name} in {CONCEPTSET_DIR}\")\n",
    "        return None\n",
    "\n",
    "    arm_data = {}\n",
    "    for folder in candidate_folders:\n",
    "        if re.search(r\"ONLINE\", folder, re.IGNORECASE):\n",
    "            continue  # skip online folders\n",
    "        folder_path = os.path.join(CONCEPTSET_DIR, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        csv_path = os.path.join(folder_path, \"includedConcepts.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "\n",
    "        arm_name = clean_arm_name(folder)\n",
    "        if arm_name and arm_name != \"UNKNOWN\":\n",
    "            arm_data[arm_name] = load_included_concepts(csv_path)\n",
    "\n",
    "    if not arm_data:\n",
    "        print(f\"‚ö†Ô∏è No valid arm concept sets found for {disease_name}\")\n",
    "        return None\n",
    "\n",
    "    # Load Final Gold Standard\n",
    "    final_df = pd.read_csv(final_file, dtype=str)\n",
    "    if \"conceptId\" not in final_df.columns:\n",
    "        print(f\"‚ö†Ô∏è conceptId missing in {final_file}\")\n",
    "        return None\n",
    "    final_concepts = set(final_df[\"conceptId\"].dropna().astype(str))\n",
    "\n",
    "    stats = []\n",
    "    # print(f\"\\nüìä Disease: {disease_name} ‚Äî {len(final_concepts)} concepts in Gold Standard\")\n",
    "\n",
    "    for arm, concepts in arm_data.items():\n",
    "        overlap = concepts & final_concepts\n",
    "        only_arm = concepts - final_concepts\n",
    "        only_final = final_concepts - concepts\n",
    "\n",
    "        stats.append({\n",
    "            \"Disease\": disease_name,\n",
    "            \"Arm\": arm,\n",
    "            \"Concepts_in_Arm\": len(concepts),\n",
    "            \"Concepts_in_Final\": len(final_concepts),\n",
    "            \"Overlap\": len(overlap),\n",
    "            \"Arm_only\": len(only_arm),\n",
    "            \"Final_only\": len(only_final),\n",
    "            \"Overlap_%\": round(100 * len(overlap) / max(len(concepts), 1), 2)\n",
    "        })\n",
    "\n",
    "        # print(f\"   - {arm}: Arm={len(concepts)} | Overlap={len(overlap)} ({round(100 * len(overlap)/max(len(concepts),1),2)}%)\")\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ Fallback: Analyze using arm_summary.csv (no concepts)\n",
    "# ---------------------------------------------------------------\n",
    "def analyze_disease_summary_fallback(disease_name, final_file):\n",
    "    \"\"\"Fallback mode using phaseA_pre_adjudication arm_summary.csv.\"\"\"\n",
    "    arm_summary_path = os.path.join(PHASEA_DIR, disease_name, \"arm_summary.csv\")\n",
    "    if not os.path.exists(arm_summary_path):\n",
    "        return None\n",
    "\n",
    "    df_summary = pd.read_csv(arm_summary_path, dtype=str)\n",
    "    final_df = pd.read_csv(final_file, dtype=str)\n",
    "\n",
    "    total_final = len(final_df[\"conceptId\"].dropna()) if \"conceptId\" in final_df.columns else 0\n",
    "    df_summary[\"Concepts_in_Final\"] = total_final\n",
    "    print(f\"üìÑ Fallback summary used for {disease_name} (no concept-level comparison)\")\n",
    "    return df_summary\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üöÄ MAIN EXECUTION\n",
    "# ---------------------------------------------------------------\n",
    "def main():\n",
    "    final_files = sorted(glob.glob(os.path.join(FINAL_GOLD_DIR, \"*_Gold_Standard_FINAL.csv\")))\n",
    "    if not final_files:\n",
    "        print(f\"‚ö†Ô∏è No final gold standard files found in {FINAL_GOLD_DIR}\")\n",
    "        return\n",
    "\n",
    "    all_results = []\n",
    "    for final_file in final_files:\n",
    "        disease_name = os.path.basename(final_file).replace(\"_Gold_Standard_FINAL.csv\", \"\")\n",
    "\n",
    "        # Try concept-level comparison first\n",
    "        summary_df = analyze_disease_conceptsets(disease_name, final_file)\n",
    "\n",
    "        # Fallback: use pre-adjudication summary\n",
    "        if summary_df is None:\n",
    "            summary_df = analyze_disease_summary_fallback(disease_name, final_file)\n",
    "\n",
    "        if summary_df is not None:\n",
    "            all_results.append(summary_df)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No valid data found for {disease_name}\")\n",
    "\n",
    "    if all_results:\n",
    "        combined = pd.concat(all_results, ignore_index=True)\n",
    "        combined.to_csv(OUTPUT_PATH, index=False)\n",
    "        # print(f\"\\n‚úÖ Saved comparison summary ‚Üí {OUTPUT_PATH}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No comparisons generated.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß† 6.2.1 ‚Äî Primary Objective Analysis (Weighted F1 vs TGS)\n",
    "# ==========================================================\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß≠ PATH CONFIGURATION\n",
    "# --------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, \"..\", \"ConceptSets\"))\n",
    "OUTPUT_PATH = os.path.join(SCRIPT_DIR, \"results\", \"gold_standard_primary_metrics.csv\")\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "FINAL_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseA_gold_final\")\n",
    "# -------------------------------------------------------------\n",
    "# üß© Helper: Clean Arm Name\n",
    "# -------------------------------------------------------------\n",
    "def clean_arm_name(folder_name):\n",
    "    \"\"\"Extract standardized arm name like AI1, H1, etc.\"\"\"\n",
    "    if re.search(r\"\\[S\\d+\\]\", folder_name, re.IGNORECASE):\n",
    "        return None\n",
    "\n",
    "    match = re.search(r\"(AI\\d+|H\\d+|HUMAN|CLINICIAN|REVIEWER|EXPERT|MANUAL)\", folder_name, re.IGNORECASE)\n",
    "    if match:\n",
    "        name = match.group(1).upper()\n",
    "        return \"H1\" if name in [\"HUMAN\", \"CLINICIAN\", \"REVIEWER\", \"EXPERT\", \"MANUAL\"] else name\n",
    "\n",
    "    parts = re.findall(r\"\\[([A-Za-z0-9]+)\\]\", folder_name)\n",
    "    if parts:\n",
    "        return parts[-1].upper()\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üß© Helper: Load includedConcepts.csv\n",
    "# -------------------------------------------------------------\n",
    "def load_included_concepts(file_path):\n",
    "    \"\"\"Return Concept IDs as a clean set.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, dtype=str)\n",
    "        cols = [c.strip() for c in df.columns]\n",
    "        norm_map = {re.sub(r\"[^a-z0-9]\", \"\", c.lower()): c for c in cols}\n",
    "\n",
    "        chosen_col = (\n",
    "            norm_map.get(\"conceptid\")\n",
    "            or next((orig for norm, orig in norm_map.items() if \"concept\" in norm and \"id\" in norm and \"set\" not in norm), None)\n",
    "        )\n",
    "        if not chosen_col:\n",
    "            print(f\"‚ö†Ô∏è No Concept ID column found in {file_path}\")\n",
    "            return set()\n",
    "\n",
    "        vals = df[chosen_col].dropna().astype(str).str.strip().str.replace(r\"\\.0+$\", \"\", regex=True)\n",
    "        return set(vals[vals != \"\"])\n",
    "    except Exception as e:\n",
    "        print(f\"[load_included_concepts] Error reading {file_path}: {e}\")\n",
    "        return set()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üß© Weighted Metrics\n",
    "# -------------------------------------------------------------\n",
    "def compute_weighted_f1(arm_concepts, gold_standard, record_count_df):\n",
    "    \"\"\"Compute weighted precision, recall, and F1.\"\"\"\n",
    "    df = record_count_df.copy()\n",
    "    df[\"record_count\"] = pd.to_numeric(df[\"record_count\"], errors=\"coerce\").fillna(0)\n",
    "    df = df[df[\"conceptId\"].isin(set(arm_concepts) | set(gold_standard))]\n",
    "\n",
    "    df[\"TP\"] = df[\"conceptId\"].isin(arm_concepts & gold_standard)\n",
    "    df[\"FP\"] = df[\"conceptId\"].isin(arm_concepts - gold_standard)\n",
    "    df[\"FN\"] = df[\"conceptId\"].isin(gold_standard - arm_concepts)\n",
    "\n",
    "    WTP = df.loc[df[\"TP\"], \"record_count\"].sum()\n",
    "    WFP = df.loc[df[\"FP\"], \"record_count\"].sum()\n",
    "    WFN = df.loc[df[\"FN\"], \"record_count\"].sum()\n",
    "\n",
    "    P_W = WTP / (WTP + WFP) if (WTP + WFP) else np.nan\n",
    "    R_W = WTP / (WTP + WFN) if (WTP + WFN) else np.nan\n",
    "    F1_W = (2 * P_W * R_W / (P_W + R_W)) if (P_W + R_W) else np.nan\n",
    "\n",
    "    return {\"WTP\": WTP, \"WFP\": WFP, \"WFN\": WFN, \"P_W\": P_W, \"R_W\": R_W, \"F1_W\": F1_W}\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üß© Unweighted Metrics\n",
    "# -------------------------------------------------------------\n",
    "def compute_unweighted_PRF(arm_concepts, gold_standard):\n",
    "    \"\"\"Compute unweighted precision, recall, F1.\"\"\"\n",
    "    tp = len(set(arm_concepts) & set(gold_standard))\n",
    "    fp = len(set(arm_concepts) - set(gold_standard))\n",
    "    fn = len(set(gold_standard) - set(arm_concepts))\n",
    "\n",
    "    P = tp / (tp + fp) if (tp + fp) else np.nan\n",
    "    R = tp / (tp + fn) if (tp + fn) else np.nan\n",
    "    F1 = (2 * P * R / (P + R)) if (P + R) else np.nan\n",
    "\n",
    "    return {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"Precision\": P, \"Recall\": R, \"F1\": F1}\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üß© Compare all arms vs Gold Standard FINAL\n",
    "# -------------------------------------------------------------\n",
    "def analyze_disease_vs_final(prefix, final_file, record_count_df):\n",
    "    \"\"\"Compare all arms for a given disease prefix.\"\"\"\n",
    "    candidate_folders = [\n",
    "        f for f in os.listdir(ROOT_DIR)\n",
    "        if re.search(rf\"\\[{prefix}\\]\", f, re.IGNORECASE)\n",
    "    ]\n",
    "\n",
    "    if not candidate_folders:\n",
    "        print(f\"‚ö†Ô∏è No folders found for prefix {prefix} in ConceptSets/\")\n",
    "        return None\n",
    "\n",
    "    # Load Final Gold Standard\n",
    "    final_df = pd.read_csv(final_file, dtype=str)\n",
    "    final_concepts = set(final_df[\"conceptId\"].dropna().astype(str))\n",
    "    gold_count = len(final_concepts)\n",
    "\n",
    "    stats = []\n",
    "    for folder in sorted(candidate_folders):\n",
    "        folder_path = os.path.join(ROOT_DIR, folder)\n",
    "        csv_path = os.path.join(folder_path, \"includedConcepts.csv\")\n",
    "        if not os.path.exists(csv_path):\n",
    "            continue\n",
    "\n",
    "        arm_name = clean_arm_name(folder)\n",
    "        if not arm_name or arm_name == \"UNKNOWN\":\n",
    "            continue\n",
    "\n",
    "        arm_concepts = load_included_concepts(csv_path)\n",
    "        if not arm_concepts:\n",
    "            continue\n",
    "\n",
    "        overlap = arm_concepts & final_concepts\n",
    "        jaccard = len(overlap) / len(arm_concepts | final_concepts) if arm_concepts or final_concepts else np.nan\n",
    "\n",
    "        weighted = compute_weighted_f1(arm_concepts, final_concepts, record_count_df)\n",
    "        unweighted = compute_unweighted_PRF(arm_concepts, final_concepts)\n",
    "\n",
    "        stats.append({\n",
    "            \"Disease\": prefix,\n",
    "            \"Arm\": arm_name,\n",
    "            \"Concepts_in_Arm\": len(arm_concepts),\n",
    "            \"Concepts_in_GoldStandard\": gold_count,\n",
    "            \"Overlap\": len(overlap),\n",
    "            \"Jaccard_Similarity\": round(jaccard, 3),\n",
    "\n",
    "            # Weighted\n",
    "            \"WTP\": weighted[\"WTP\"], \"WFP\": weighted[\"WFP\"], \"WFN\": weighted[\"WFN\"],\n",
    "            \"Weighted_Precision\": round(weighted[\"P_W\"], 3) if pd.notna(weighted[\"P_W\"]) else \"\",\n",
    "            \"Weighted_Recall\": round(weighted[\"R_W\"], 3) if pd.notna(weighted[\"R_W\"]) else \"\",\n",
    "            \"Weighted_F1\": round(weighted[\"F1_W\"], 3) if pd.notna(weighted[\"F1_W\"]) else \"\",\n",
    "\n",
    "            # Unweighted\n",
    "            \"TP\": unweighted[\"TP\"], \"FP\": unweighted[\"FP\"], \"FN\": unweighted[\"FN\"],\n",
    "            \"Unweighted_Precision\": round(unweighted[\"Precision\"], 3) if pd.notna(unweighted[\"Precision\"]) else \"\",\n",
    "            \"Unweighted_Recall\": round(unweighted[\"Recall\"], 3) if pd.notna(unweighted[\"Recall\"]) else \"\",\n",
    "            \"Unweighted_F1\": round(unweighted[\"F1\"], 3) if pd.notna(unweighted[\"F1\"]) else \"\"\n",
    "        })\n",
    "\n",
    "\n",
    "    if not stats:\n",
    "        print(f\"‚ö†Ô∏è No valid concept sets for {prefix}\")\n",
    "        return None\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üöÄ MAIN EXECUTION\n",
    "# -------------------------------------------------------------\n",
    "def main():\n",
    "    record_count_path = os.path.join(SCRIPT_DIR, \"ConceptRecordCounts.csv\")\n",
    "    if not os.path.exists(record_count_path):\n",
    "        print(\"‚ö†Ô∏è ConceptRecordCounts.csv not found ‚Äî weighted metrics may be skipped.\")\n",
    "        record_count_df = pd.DataFrame(columns=[\"conceptId\", \"record_count\"])\n",
    "    else:\n",
    "        record_count_df = pd.read_csv(record_count_path, dtype=str)\n",
    "\n",
    "    \n",
    "    final_files = sorted(glob.glob(os.path.join(FINAL_DIR, \"*_Gold_Standard_FINAL.csv\")))\n",
    "   \n",
    "    all_results = []\n",
    "\n",
    "    if not final_files:\n",
    "        print(\"‚ö†Ô∏è No Gold Standard FINAL files found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üß© Found {len(final_files)} FINAL gold standard files.\\n\")\n",
    "\n",
    "    for final_file in final_files:\n",
    "        disease_prefix = re.search(r\"(C\\d+)\", os.path.basename(final_file))\n",
    "        if not disease_prefix:\n",
    "            print(f\"‚ö†Ô∏è Could not detect prefix in {final_file}\")\n",
    "            continue\n",
    "        prefix = disease_prefix.group(1)\n",
    "\n",
    "        summary_df = analyze_disease_vs_final(prefix, final_file, record_count_df)\n",
    "        if summary_df is not None:\n",
    "            all_results.append(summary_df)\n",
    "\n",
    "    if all_results:\n",
    "        combined = pd.concat(all_results, ignore_index=True)\n",
    "        combined = combined.sort_values(by=[\"Disease\", \"Arm\"])\n",
    "        combined.to_csv(OUTPUT_PATH, index=False)\n",
    "        print(f\"\\n‚úÖ Saved weighted F1 summary ‚Üí {OUTPUT_PATH}\")\n",
    "        print(f\"   Rows: {combined.shape[0]} | Diseases: {combined['Disease'].nunique()}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No data processed ‚Äî check ConceptSets and FINAL files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.read_csv(os.path.join(SCRIPT_DIR, \"results\",'gold_standard_primary_metrics.csv'),dtype=str)\n",
    "# summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full summary (if not already loaded)\n",
    "\n",
    "\n",
    "RESULTS_DIR = os.path.join(SCRIPT_DIR, \"results\")\n",
    "INPUT_PATH = os.path.join(RESULTS_DIR, \"gold_standard_primary_metrics.csv\")\n",
    "\n",
    "OUTPUT_DIR = os.path.join(RESULTS_DIR, \"phaseB_primary_objectives\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"phase2_primary_objectives_summary_short.csv\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üìò LOAD INPUT DATA\n",
    "# --------------------------------------------\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(f\"‚ùå Input file not found: {INPUT_PATH}\\nPlease run the weighted F1 analysis first.\")\n",
    "\n",
    "summary_df = pd.read_csv(INPUT_PATH, dtype=str)\n",
    "# print(f\"üìÑ Loaded full summary ‚Üí {INPUT_PATH} ({len(summary_df):,} rows)\")\n",
    "\n",
    "essential_cols = [\n",
    "    \"Disease\", \"Arm\",\n",
    "    \"WTP\", \"WFP\", \"WFN\",\n",
    "    \"Weighted_Precision\", \"Weighted_Recall\", \"Weighted_F1\",\n",
    "    \"Unweighted_F1\"\n",
    "]\n",
    "\n",
    "\n",
    "# Ensure all columns exist before selection\n",
    "missing = [col for col in essential_cols if col not in summary_df.columns]\n",
    "if missing:\n",
    "    print(f\"‚ö†Ô∏è Missing expected columns: {missing} ‚Äî will fill with blanks.\")\n",
    "    for col in missing:\n",
    "        summary_df[col] = \"\"\n",
    "\n",
    "summary_short = summary_df[essential_cols].copy()\n",
    "\n",
    "# Convert numeric fields to 3-decimal floats (if applicable)\n",
    "numeric_cols = [\"Weighted_Precision\", \"Weighted_Recall\", \"Weighted_F1\", \"Unweighted_F1\"]\n",
    "for col in numeric_cols:\n",
    "    summary_short[col] = pd.to_numeric(summary_short[col], errors=\"coerce\").round(3)\n",
    "\n",
    "# --------------------------------------------\n",
    "# üíæ SAVE OUTPUT\n",
    "# --------------------------------------------\n",
    "summary_short.to_csv(OUTPUT_PATH, index=False)\n",
    "# print(f\"‚úÖ Saved condensed summary ‚Üí {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase B ‚Äî Primary Objective Analysis: Disease-Level Weighted F1 ($F1_W$) Across Diseases (6.2.1)\n",
    "\n",
    "**Analysis type:** Prevalence-Weighted F1 ($F1_W$) ‚Äî the primary outcome measure described in Section 6.2.1 of the SAP.  \n",
    "$F1_W$ represents the harmonic mean of precision and recall, weighted by each concept‚Äôs prevalence ($V_i$) in the source data.\n",
    "\n",
    "1. **Compare Human vs AI Performance (FRD 6.2.1):** Calculates the prevalence-weighted F1 for the Human arm (H1) and the mean across all AI arms (AI1‚ÄìAI4) per disease.  \n",
    "2. **Compute Differences (Œî) Between AI and Human:** Determines the difference between mean AI $F1_W$ and Human $F1_W$ to assess AI performance relative to the benchmark.  \n",
    "3. **Output Saved (CSV):** The summarized, disease-level comparison is saved in `phase1_diseaselevel_summary_pretty.csv`, capturing per-disease weighted F1 scores and AI‚ÄìHuman differences for reporting and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üß† PHASE 2 ‚Äî DISEASE-LEVEL SUMMARY (AI vs Human Weighted F1)\n",
    "# ===============================================================\n",
    "\n",
    "INPUT_PATH = os.path.join(SCRIPT_DIR, \"results\", \"gold_standard_primary_metrics.csv\")\n",
    "OUTPUT_DIR = os.path.join(SCRIPT_DIR, \"results\", \"phaseB_primary_objectives\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"phase2_diseaselevel_summary_pretty.csv\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üìò LOAD DATA\n",
    "# --------------------------------------------\n",
    "if not os.path.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(f\"‚ùå Input file not found: {INPUT_PATH}\")\n",
    "\n",
    "summary_df = pd.read_csv(INPUT_PATH, dtype=str)\n",
    "# print(f\"üìÑ Loaded weighted F1 data ‚Üí {INPUT_PATH} ({len(summary_df):,} rows)\")\n",
    "\n",
    "# Convert Weighted_F1 to numeric safely\n",
    "summary_df[\"Weighted_F1\"] = pd.to_numeric(summary_df[\"Weighted_F1\"], errors=\"coerce\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß© FILTER HUMAN (H1) AND AI (AI1‚ÄìAIn) ARMS\n",
    "# --------------------------------------------\n",
    "human_df = summary_df[summary_df[\"Arm\"].str.upper() == \"H1\"].copy()\n",
    "ai_df = summary_df[summary_df[\"Arm\"].str.upper().str.startswith(\"AI\")].copy()\n",
    "\n",
    "if human_df.empty or ai_df.empty:\n",
    "    raise ValueError(\"‚ö†Ô∏è Missing Human (H1) or AI arms in summary data ‚Äî check input file.\")\n",
    "\n",
    "# Compute mean AI Weighted F1 per disease\n",
    "ai_mean = ai_df.groupby(\"Disease\")[\"Weighted_F1\"].mean()\n",
    "\n",
    "# --------------------------------------------\n",
    "# üßÆ MERGE INTO A SINGLE SUMMARY TABLE\n",
    "# --------------------------------------------\n",
    "human_f1 = human_df.set_index(\"Disease\")[\"Weighted_F1\"].rename(\"Human_F1W\")\n",
    "ai_mean_f1 = ai_mean.rename(\"AI_Mean_F1W\")\n",
    "\n",
    "disease_summary = pd.concat([human_f1, ai_mean_f1], axis=1)\n",
    "\n",
    "# Compute Œî(AI‚ÄìH1)\n",
    "disease_summary[\"Œî(AI‚ÄìH1)\"] = disease_summary[\"AI_Mean_F1W\"] - disease_summary[\"Human_F1W\"]\n",
    "\n",
    "# --------------------------------------------\n",
    "# üßæ FORMAT & ROUND\n",
    "# --------------------------------------------\n",
    "disease_summary = (\n",
    "    disease_summary.reset_index()\n",
    "    .rename(columns={\n",
    "        \"Human_F1W\": \"Human $F1_W$\",\n",
    "        \"AI_Mean_F1W\": \"Mean AI $F1_W$\",\n",
    "        \"Œî(AI‚ÄìH1)\": \"Œî(AI‚ÄìH1)\"\n",
    "    })\n",
    "    .round(3)\n",
    "    .sort_values(by=\"Œî(AI‚ÄìH1)\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üíæ SAVE & DISPLAY\n",
    "# --------------------------------------------\n",
    "disease_summary.to_csv(OUTPUT_PATH, index=False)\n",
    "# print(f\"‚úÖ Saved AI vs Human F1 summary ‚Üí {OUTPUT_PATH}\")\n",
    "\n",
    "# print(\"\\nüìä AI vs Human (H1) Weighted F1 Summary by Disease:\")\n",
    "display(disease_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summarize by disease (mean AI vs Human)\n",
    "# summary_short = summary_df[['Disease', 'Arm', 'Weighted_F1']].copy()\n",
    "\n",
    "# # Split Human vs AI\n",
    "# human = summary_short[summary_short['Arm'] == 'H1'].set_index('Disease')\n",
    "# ai = summary_short[summary_short['Arm'].str.startswith('AI')]\n",
    "\n",
    "# # Mean AI F1 per disease\n",
    "# ai_mean = ai.groupby('Disease')['Weighted_F1'].mean().rename('AI_Mean_F1')\n",
    "\n",
    "# # Merge\n",
    "# disease_summary = human[['Weighted_F1']].rename(columns={'Weighted_F1': 'Human $F1_W$'}).join(ai_mean)\n",
    "# disease_summary['Œî(AI‚ÄìH1)'] = disease_summary['AI_Mean_F1'] - disease_summary['Mean AI $F1_W$']\n",
    "\n",
    "# # Save concise file\n",
    "# disease_summary.to_csv(\"phase1_primaryobjectivesummary_diseaselevel.csv\")\n",
    "# display(disease_summary.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------**Cross-Disease Comparative Summary**----------------\n",
    "\n",
    "**Cross-Disease Comparative Analysis of AI vs Human Workflows (Mean Œî(AI‚ÄìH1) in Prevalence-Weighted F1)**\n",
    "\n",
    "This table summarizes the mean difference (Œî) in prevalence-weighted F1 ($F1_W$) between each AI workflow (AI1‚ÄìAI4) and the Human workflow, averaged across all diseases.\n",
    "**Higher positive Œî** values indicate better average alignment of the **AI workflow** with the **True Gold Standard (TGS)** compared to the Human reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = summary_df.pivot(index='Disease', columns='Arm', values='Weighted_F1')\n",
    "comparisons = []\n",
    "for ai in ['AI1','AI2','AI3','AI4']:\n",
    "    diffs = pivot[ai] - pivot['H1']\n",
    "    comparisons.append({\n",
    "        'AI_arm': ai,\n",
    "        'Mean_Œî': diffs.mean(),\n",
    "        '95%_CI_lower': diffs.mean() - 1.96*diffs.std()/np.sqrt(len(diffs)),\n",
    "        '95%_CI_upper': diffs.mean() + 1.96*diffs.std()/np.sqrt(len(diffs))\n",
    "    })\n",
    "comparisons = pd.DataFrame(comparisons)\n",
    "comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"üîç Checking disease‚Äìarm availability:\")\n",
    "# pivot_check = summary_df.pivot_table(index='Disease', columns='Arm', values='Weighted_F1', aggfunc='count', fill_value=0)\n",
    "# print(pivot_check)\n",
    "# print(\"\\nNumber of diseases with complete arm data:\", (pivot_check > 0).all(axis=1).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase B ‚Äî Paired Comparative Analysis with Permutation FWER Correction (6.2.1)\n",
    "\n",
    "1. **Descriptive Statistics per Arm (FRD 6.2.1, SAP 2.1.3):** Calculates mean, median, standard deviation, min, max, and 95% CI for Weighted F1 ($F1_W$) of Human (H1) and AI arms across diseases with complete data.  \n",
    "2. **Paired Comparisons & Non-Inferiority Testing:** Performs paired t-tests or Wilcoxon signed-rank tests comparing each AI arm to H1, including bootstrapped 95% CIs and evaluation of non-inferiority with a margin Œ¥=0.05.  \n",
    "3. **Permutation-Based Familywise Error Rate (FWER):** Implements exact sign-flip permutation tests to adjust for multiple comparisons, integrating results into `paired_analysis_summary_with_permutation.csv`; this supports robust inference on AI vs. Human performance differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üìä PRIMARY STATISTICS (SAP 2.1.3) ‚Äî with permutation FWER correction\n",
    "# ==========================================================\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# --- 0Ô∏è‚É£ Clean and prepare data -------------------------------------------\n",
    "summary_df['Arm'] = (\n",
    "    summary_df['Arm'].astype(str).str.strip().str.upper()\n",
    "    .replace({'HUMAN': 'H1', 'H1.0': 'H1'})\n",
    ")\n",
    "summary_df['Weighted_F1'] = pd.to_numeric(summary_df['Weighted_F1'], errors='coerce')\n",
    "\n",
    "# Drop diseases missing any arm‚Äôs Weighted_F1 (must have H1 + 4 AI arms)\n",
    "complete_diseases = (\n",
    "    summary_df.groupby('Disease')['Weighted_F1']\n",
    "    .apply(lambda x: x.notna().sum() == 5)\n",
    ")\n",
    "complete_diseases = complete_diseases[complete_diseases].index\n",
    "summary_df = summary_df[summary_df['Disease'].isin(complete_diseases)].copy()\n",
    "\n",
    "print(f\"‚úÖ Using {len(complete_diseases)} diseases with complete arms: {list(complete_diseases)}\")\n",
    "\n",
    "# --- 1Ô∏è‚É£ Descriptive statistics per arm -----------------------------------\n",
    "desc = (\n",
    "    summary_df\n",
    "    .groupby('Arm')['Weighted_F1']\n",
    "    .agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "    .reset_index()\n",
    ")\n",
    "desc['N'] = summary_df['Disease'].nunique()\n",
    "desc['95CI_low']  = desc['mean'] - 1.96 * desc['std'] / np.sqrt(desc['N'])\n",
    "desc['95CI_high'] = desc['mean'] + 1.96 * desc['std'] / np.sqrt(desc['N'])\n",
    "desc.to_csv(\"descriptive_statistics_by_arm.csv\", index=False)\n",
    "# print(\"\\n‚úÖ Descriptive statistics saved to descriptive_statistics_by_arm.csv\")\n",
    "\n",
    "\n",
    "# --- 2Ô∏è‚É£ Paired comparative analysis --------------------------------------\n",
    "def paired_compare(df, control='H1', delta=0.05):\n",
    "    diseases = df['Disease'].unique()\n",
    "    ai_arms = [a for a in df['Arm'].unique() if a != control]\n",
    "    out = []\n",
    "\n",
    "    for ai in ai_arms:\n",
    "        diffs = []\n",
    "        for d in diseases:\n",
    "            ddf = df[df['Disease'] == d]\n",
    "            try:\n",
    "                f_ai = float(ddf.loc[ddf['Arm'] == ai, 'Weighted_F1'].iloc[0])\n",
    "                f_h  = float(ddf.loc[ddf['Arm'] == control, 'Weighted_F1'].iloc[0])\n",
    "                if np.isfinite(f_ai) and np.isfinite(f_h):\n",
    "                    diffs.append(f_ai - f_h)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        diffs = np.array(diffs)\n",
    "        n = len(diffs)\n",
    "        if n < 2:\n",
    "            continue  # too few pairs to test\n",
    "\n",
    "        mean_diff = np.mean(diffs)\n",
    "        sd_diff   = np.std(diffs, ddof=1)\n",
    "        shapiro_p = stats.shapiro(diffs).pvalue if n >= 3 else np.nan\n",
    "\n",
    "        # choose test\n",
    "        if np.isnan(shapiro_p) or shapiro_p <= 0.05:\n",
    "            try:\n",
    "                t_stat, p_val = stats.wilcoxon(diffs)\n",
    "                test_used = \"Wilcoxon\"\n",
    "            except ValueError:\n",
    "                t_stat, p_val = np.nan, np.nan\n",
    "                test_used = \"Wilcoxon (invalid)\"\n",
    "        else:\n",
    "            t_stat, p_val = stats.ttest_rel(diffs, np.zeros_like(diffs))\n",
    "            test_used = \"Paired t-test\"\n",
    "\n",
    "        # bootstrap CI\n",
    "        rng = np.random.default_rng(42)\n",
    "        boot = [np.mean(rng.choice(diffs, n, replace=True)) for _ in range(5000)]\n",
    "        ci_low, ci_high = np.percentile(boot, [2.5, 97.5])\n",
    "        non_inferior = ci_low > -delta\n",
    "\n",
    "        out.append({\n",
    "            'AI_Arm': ai,\n",
    "            'N_Diseases': n,\n",
    "            'Mean_Diff_F1W': round(mean_diff, 3),\n",
    "            'SD_Diff': round(sd_diff, 3),\n",
    "            'Shapiro_p': round(shapiro_p, 3) if not np.isnan(shapiro_p) else np.nan,\n",
    "            'Test_Used': test_used,\n",
    "            'p_value': round(p_val, 4) if p_val is not None and np.isfinite(p_val) else np.nan,\n",
    "            '95CI_Low': round(ci_low, 3),\n",
    "            '95CI_High': round(ci_high, 3),\n",
    "            'NonInferior(Œ¥=0.05)': non_inferior\n",
    "        })\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "paired_df = paired_compare(summary_df, control='H1', delta=0.05)\n",
    "paired_df.to_csv(\"paired_analysis_summary.csv\", index=False)\n",
    "# print(\"\\n‚úÖ Paired comparison saved to paired_analysis_summary.csv\")\n",
    "\n",
    "# --- 3Ô∏è‚É£ Exact permutation (sign-flip) familywise correction --------------\n",
    "def permutation_max_stat_familywise(summary_df, control='H1'):\n",
    "    diseases = sorted(summary_df['Disease'].unique())\n",
    "    ai_arms = [a for a in sorted(summary_df['Arm'].unique()) if a != control]\n",
    "\n",
    "    # build matrix\n",
    "    mat = {arm: np.array([\n",
    "        summary_df.loc[(summary_df['Disease']==d)&(summary_df['Arm']==arm),\n",
    "                       'Weighted_F1'].astype(float).iloc[0]\n",
    "        if not summary_df.loc[(summary_df['Disease']==d)&(summary_df['Arm']==arm)].empty\n",
    "        else np.nan\n",
    "        for d in diseases]) for arm in ai_arms+[control]}\n",
    "\n",
    "    # keep only diseases present for all arms\n",
    "    valid_mask = np.all(~np.isnan(np.column_stack(list(mat.values()))), axis=1)\n",
    "    diseases_valid = np.array(diseases)[valid_mask]\n",
    "    n = len(diseases_valid)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"No complete disease rows across all arms\")\n",
    "\n",
    "    # compute diffs\n",
    "    diffs_matrix = np.vstack([(mat[ai][valid_mask] - mat[control][valid_mask])\n",
    "                              for ai in ai_arms])\n",
    "    n_arms = diffs_matrix.shape[0]\n",
    "\n",
    "    # observed mean diffs\n",
    "    obs_mean = diffs_matrix.mean(axis=1)\n",
    "\n",
    "    # exact sign-flip enumeration (2^n permutations)\n",
    "    sign_patterns = list(itertools.product([1, -1], repeat=n))\n",
    "    max_stats = []\n",
    "    for signs in sign_patterns:\n",
    "        s = np.array(signs)\n",
    "        means = diffs_matrix.dot(s) / n\n",
    "        max_stats.append(np.max(np.abs(means)))\n",
    "    max_stats = np.array(max_stats)\n",
    "\n",
    "    # familywise p for each AI arm\n",
    "    fw_p = [np.mean(max_stats >= abs(obs_mean[i])) for i in range(n_arms)]\n",
    "\n",
    "    perm_df = pd.DataFrame({\n",
    "        'AI_Arm': ai_arms,\n",
    "        'Observed_Mean_Diff': obs_mean,\n",
    "        'FWER_pvalue': fw_p\n",
    "    })\n",
    "    return perm_df\n",
    "\n",
    "perm_df = permutation_max_stat_familywise(summary_df, control='H1')\n",
    "paired_df = paired_df.merge(perm_df, on='AI_Arm', how='left')\n",
    "paired_df.to_csv(\"paired_analysis_summary_with_permutation.csv\", index=False)\n",
    "\n",
    "print(\"\\nüîç Paired_df check before plotting:\")\n",
    "# print(paired_df)\n",
    "\n",
    "# --- 4Ô∏è‚É£ Visualization: forest-style plot --------------------------------\n",
    "# paired_plot = paired_df.replace([np.inf, -np.inf], np.nan)\n",
    "# paired_plot = paired_plot.dropna(subset=['Mean_Diff_F1W', '95CI_Low', '95CI_High'])\n",
    "\n",
    "# if paired_plot.empty:\n",
    "#     print(\"‚ö†Ô∏è No valid arms with finite CI values. Possibly missing H1 or identical scores.\")\n",
    "# else:\n",
    "#     fig, ax = plt.subplots(figsize=(8, 5))\n",
    "#     ax.errorbar(\n",
    "#         paired_plot['AI_Arm'],\n",
    "#         paired_plot['Mean_Diff_F1W'],\n",
    "#         yerr=[\n",
    "#             paired_plot['Mean_Diff_F1W'] - paired_plot['95CI_Low'],\n",
    "#             paired_plot['95CI_High'] - paired_plot['Mean_Diff_F1W']\n",
    "#         ],\n",
    "#         fmt='o', capsize=5, color='navy'\n",
    "#     )\n",
    "#     ax.axhline(0, color='gray', ls='--', label='No difference')\n",
    "#     ax.axhline(-0.05, color='red', ls=':', label='Non-inferiority margin (‚àí0.05)')\n",
    "#     for i, row in paired_plot.iterrows():\n",
    "#         lbl = (\n",
    "#             f\"{row['Mean_Diff_F1W']:.2f} \"\n",
    "#             f\"[{row['95CI_Low']:.2f}, {row['95CI_High']:.2f}]\\n\"\n",
    "#             f\"p={row['p_value'] if pd.notna(row['p_value']) else 'NA'}, \"\n",
    "#             f\"FWER={row['FWER_pvalue'] if pd.notna(row['FWER_pvalue']) else 'NA'}\"\n",
    "#         )\n",
    "#         ax.text(row['AI_Arm'], row['Mean_Diff_F1W'] + 0.02, lbl,\n",
    "#                 ha='center', va='bottom', fontsize=9)\n",
    "#     ax.set_ylabel(\"Mean difference in Weighted F1 (AI ‚Äì Human)\")\n",
    "#     ax.set_title(\"Paired comparison across clinical ideas\\n(Exact permutation FWER adjusted)\")\n",
    "#     ax.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"paired_f1_differences_permutation.png\", dpi=300)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase B ‚Äî Final Primary Objective Analysis Summary (SAP 6.2.1)\n",
    "\n",
    "1. **Comprehensive Summary Across Metrics:** Combines descriptive statistics, disease-level Œî(AI‚ÄìH1) comparisons, and paired inferential results for all AI and Human (H1) arms, using Weighted F1 ($F1_W$) as the primary outcome measure (FRD 6.2.1).  \n",
    "2. **Non-Inferiority Assessment:** Evaluates each AI arm against H1 with 95% CIs, FWER-adjusted p-values, and interpretation of non-inferiority using Œ¥ = 0.05 to identify AI workflows performing comparably to humans.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üìÑ SAP 6.2.1 ‚Äî Final Primary Objective Analysis Summary\n",
    "# ==========================================================\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"üìä SAP 6.2.1 ‚Äî Primary Objective Analysis Summary\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # 1Ô∏è‚É£ DESCRIPTIVE STATISTICS ---------------------------------------------------\n",
    "# print(\"\\n1Ô∏è‚É£ DESCRIPTIVE STATISTICS (Weighted F1 per Arm)\")\n",
    "# try:\n",
    "#     print(desc[['Arm', 'mean', 'median', 'std', 'min', 'max', '95CI_low', '95CI_high']].round(3))\n",
    "# except Exception as e:\n",
    "#     print(\"‚ö†Ô∏è Descriptive summary unavailable:\", e)\n",
    "\n",
    "# 2Ô∏è‚É£ DISEASE-LEVEL Œî(AI‚ÄìH1) COMPARISON ---------------------------------------\n",
    "# try:\n",
    "#     pivot = summary_df.pivot(index='Disease', columns='Arm', values='Weighted_F1')\n",
    "#     pivot['AI_Mean'] = pivot[['AI1', 'AI2', 'AI3', 'AI4']].mean(axis=1)\n",
    "#     pivot['Œî(AI‚ÄìH1)'] = pivot['AI_Mean'] - pivot['H1']\n",
    "#     print(\"\\n2Ô∏è‚É£ DISEASE-LEVEL COMPARISON (Human vs Mean AI Weighted F1)\")\n",
    "#     print(pivot[['H1', 'AI_Mean', 'Œî(AI‚ÄìH1)']].round(3))\n",
    "# except Exception as e:\n",
    "#     print(\"‚ö†Ô∏è Could not compute per-disease Œî(AI‚ÄìH1):\", e)\n",
    "\n",
    "# # 3Ô∏è‚É£ PAIRED INFERENTIAL ANALYSIS ---------------------------------------------\n",
    "# print(\"\\n3Ô∏è‚É£ PAIRED INFERENTIAL RESULTS (AI vs Human)\")\n",
    "# try:\n",
    "#     print(paired_df[['AI_Arm', 'N_Diseases', 'Mean_Diff_F1W', '95CI_Low', '95CI_High',\n",
    "#                      'Test_Used', 'p_value', 'FWER_pvalue', 'NonInferior(Œ¥=0.05)']])\n",
    "# except Exception as e:\n",
    "#     print(\"‚ö†Ô∏è Paired inferential results unavailable:\", e)\n",
    "\n",
    "# # 4Ô∏è‚É£ NON-INFERIORITY INTERPRETATION ------------------------------------------\n",
    "# print(\"\\n4Ô∏è‚É£ NON-INFERIORITY INTERPRETATION (Œ¥ = 0.05)\")\n",
    "# try:\n",
    "#     for _, row in paired_df.iterrows():\n",
    "#         status = \"‚úÖ Non-inferior\" if row['95CI_Low'] > -0.05 else \"‚ùå Not non-inferior\"\n",
    "#         print(f\"{row['AI_Arm']}: Œî={row['Mean_Diff_F1W']:.3f}, \"\n",
    "#               f\"95% CI=({row['95CI_Low']:.3f}, {row['95CI_High']:.3f}), \"\n",
    "#               f\"FWER p={row['FWER_pvalue']:.3f}, {status}\")\n",
    "# except Exception as e:\n",
    "#     print(\"‚ö†Ô∏è Non-inferiority summary unavailable:\", e)\n",
    "\n",
    "# 5Ô∏è‚É£ SAVE ALL SUMMARIES ------------------------------------------------------\n",
    "# print(\"\\n5Ô∏è‚É£ EXPORTING RESULTS\")\n",
    "# try:\n",
    "#     with pd.ExcelWriter(\"SAP_6_2_1_PrimaryObjectiveAnalysis.xlsx\") as writer:\n",
    "#         desc.to_excel(writer, sheet_name=\"Descriptive_Stats\", index=False)\n",
    "#         pivot[['H1', 'AI_Mean', 'Œî(AI‚ÄìH1)']].to_excel(writer, sheet_name=\"Disease_Comparison\")\n",
    "#         paired_df.to_excel(writer, sheet_name=\"Paired_Inferential\", index=False)\n",
    "#     # print(\"‚úÖ Saved: SAP_6_2_1_PrimaryObjectiveAnalysis.xlsx\")\n",
    "# except Exception as e:\n",
    "#     print(\"‚ö†Ô∏è Export failed:\", e)\n",
    "\n",
    "# print(\"\\n‚úÖ SAP 6.2.1 summary complete.\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Inferential Analysis ‚Äî Paired Comparison of AI vs Human Workflows (Weighted F1 Œî with FWER Correction)**\n",
    "\n",
    "This table presents the paired statistical comparison of each AI workflow against the Human workflow across all clinical ideas\n",
    "\n",
    "For each AI arm, the mean difference in prevalence-weighted F1 ($Œî = AI ‚Äì H1$), standard deviation, 95 % bootstrap confidence interval, normality check, chosen paired test, and permutation-adjusted (FWER) p-values are reported.\n",
    "Positive mean Œî values indicate higher average alignment of the AI workflow with the True Gold Standard (TGS) relative to the Human reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the essential columns for SAP reporting\n",
    "paired_df = paired_df[[\n",
    "    'AI_Arm',\n",
    "    'Mean_Diff_F1W',\n",
    "    '95CI_Low',\n",
    "    '95CI_High',\n",
    "    'Test_Used',\n",
    "    'p_value',\n",
    "    'FWER_pvalue',\n",
    "    'NonInferior(Œ¥=0.05)'\n",
    "]]\n",
    "\n",
    "# Display the first 10 rows\n",
    "paired_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Phase B ‚Äî Derived Metrics & Weighted F1 Integration (6.2.1)\n",
    "\n",
    "1. **Compute Standard Metrics (FRD 6.2.1):** Calculates per-arm precision, recall, and unweighted F1 based on overlap with the True Gold Standard (TGS), along with coverage rate (recall) and specificity (inverse of false positives).  \n",
    "2. **Weighted F1 Calculation:** Integrates prevalence-weighted F1 ($F1_W$) using `ConceptRecordCounts.csv` if not already present, reflecting the primary outcome measure from Section 6.2.1 of the SAP.  \n",
    "3. **Output Saved (CSV):** The enhanced summary, including all derived metrics and weighted F1, is saved as `gold_standard_comparison_summary_with_metrics.csv` for downstream analysis and reporting.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üßÆ GOLD STANDARD COMPARISON ‚Äî FINAL VERSION (No ONLINE folders)\n",
    "# ===============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß≠ PATH SETUP\n",
    "# --------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, '..', 'ConceptSets'))\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    alt_root = os.path.join(SCRIPT_DIR, 'ConceptSets')\n",
    "    if os.path.exists(alt_root):\n",
    "        ROOT_DIR = os.path.abspath(alt_root)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"‚ö†Ô∏è ConceptSets folder not found.\")\n",
    "\n",
    "RESULTS_DIR = os.path.join(SCRIPT_DIR, \"results\")\n",
    "GOLD_FINAL_DIR = os.path.join(RESULTS_DIR, \"phaseA_gold_final\")\n",
    "INPUT_PATH = os.path.join(RESULTS_DIR, \"gold_standard_comparison_summary.csv\")\n",
    "RECORD_COUNT_PATH = os.path.join(SCRIPT_DIR, \"ConceptRecordCounts.csv\")\n",
    "OUTPUT_PATH = os.path.join(RESULTS_DIR, \"gold_standard_comparison_summary_with_metrics.csv\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# üßπ Helper: Clean folder names\n",
    "# --------------------------------------------\n",
    "def strip_numeric_prefix(name: str) -> str:\n",
    "    \"\"\"Remove leading numeric prefixes like '0152-' or '0031_'.\"\"\"\n",
    "    return re.sub(r\"^[\\d\\-\\_]+\", \"\", name).strip()\n",
    "\n",
    "# --------------------------------------------\n",
    "# üìò LOAD INPUT FILES\n",
    "# --------------------------------------------\n",
    "summary_df = pd.read_csv(INPUT_PATH)\n",
    "record_count_df = pd.read_csv(RECORD_COUNT_PATH, dtype=str)\n",
    "record_count_df[\"record_count\"] = pd.to_numeric(record_count_df[\"record_count\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "for col in [\"Overlap\", \"Arm_only\", \"GoldStandard_only\"]:\n",
    "    if col not in summary_df.columns:\n",
    "        summary_df[col] = 0\n",
    "    summary_df[col] = pd.to_numeric(summary_df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# --------------------------------------------\n",
    "# üß© Weighted F1 function\n",
    "# --------------------------------------------\n",
    "def compute_weighted_f1(arm_concepts, gold_standard, record_count_df):\n",
    "    df = record_count_df[record_count_df[\"conceptId\"].isin(set(arm_concepts) | set(gold_standard))].copy()\n",
    "    if df.empty:\n",
    "        return np.nan\n",
    "\n",
    "    df[\"TP\"] = df[\"conceptId\"].isin(arm_concepts & gold_standard)\n",
    "    df[\"FP\"] = df[\"conceptId\"].isin(arm_concepts - gold_standard)\n",
    "    df[\"FN\"] = df[\"conceptId\"].isin(gold_standard - arm_concepts)\n",
    "\n",
    "    WTP = df.loc[df[\"TP\"], \"record_count\"].sum()\n",
    "    WFP = df.loc[df[\"FP\"], \"record_count\"].sum()\n",
    "    WFN = df.loc[df[\"FN\"], \"record_count\"].sum()\n",
    "\n",
    "    P_W = WTP / (WTP + WFP) if (WTP + WFP) else np.nan\n",
    "    R_W = WTP / (WTP + WFN) if (WTP + WFN) else np.nan\n",
    "    F1_W = (2 * P_W * R_W / (P_W + R_W)) if (P_W + R_W) else np.nan\n",
    "    return round(F1_W, 3)\n",
    "\n",
    "# --------------------------------------------\n",
    "# üöÄ MAIN LOOP\n",
    "# --------------------------------------------\n",
    "weighted_f1_scores = []\n",
    "\n",
    "for _, row in summary_df.iterrows():\n",
    "    disease = str(row[\"Disease\"])\n",
    "    arm = str(row[\"Arm\"]).upper()\n",
    "\n",
    "    # --- Locate Gold Standard file ---\n",
    "    gold_paths = [\n",
    "        os.path.join(GOLD_FINAL_DIR, f\"{disease}\", f\"{disease}_Gold_Standard_FINAL.csv\"),\n",
    "        os.path.join(GOLD_FINAL_DIR, f\"{disease}_Gold_Standard_FINAL.csv\"),\n",
    "    ]\n",
    "    gold_path = next((p for p in gold_paths if os.path.exists(p)), None)\n",
    "    if not gold_path:\n",
    "        print(f\"‚ö†Ô∏è Skipping {disease}-{arm}: Gold Standard not found.\")\n",
    "        weighted_f1_scores.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    gold_df = pd.read_csv(gold_path, dtype=str)\n",
    "    gold_df[\"conceptId\"] = gold_df[\"conceptId\"].astype(str).str.strip().str.replace(r'\\.0+$', '', regex=True)\n",
    "    gold_concepts = set(gold_df[\"conceptId\"])\n",
    "\n",
    "    # --- Locate matching ConceptSets folder (flat structure) ---\n",
    "    matched_folder = None\n",
    "    for folder in os.listdir(ROOT_DIR):\n",
    "        clean_name = strip_numeric_prefix(folder)\n",
    "        if re.search(rf\"\\[{disease.split('_')[0]}\\]\", clean_name, re.IGNORECASE) and \\\n",
    "           re.search(rf\"\\[{arm}\\]\", clean_name, re.IGNORECASE):\n",
    "            # ‚õî Skip online folders entirely\n",
    "            if re.search(r'ONLINE', clean_name, re.IGNORECASE):\n",
    "                matched_folder = None\n",
    "                break\n",
    "            matched_folder = os.path.join(ROOT_DIR, folder)\n",
    "            break\n",
    "\n",
    "    if not matched_folder:\n",
    "        # print(f\"‚ö†Ô∏è Skipping {disease}-{arm}: No valid (non-online) folder found in ConceptSets.\")\n",
    "        weighted_f1_scores.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    arm_path = os.path.join(matched_folder, \"includedConcepts.csv\")\n",
    "    if not os.path.exists(arm_path):\n",
    "        print(f\"‚ö†Ô∏è Missing includedConcepts.csv for {arm} in {matched_folder}\")\n",
    "        weighted_f1_scores.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    arm_df = pd.read_csv(arm_path, dtype=str)\n",
    "    concept_col = next((c for c in arm_df.columns if \"conceptid\" in c.lower()), arm_df.columns[0])\n",
    "    arm_df[concept_col] = arm_df[concept_col].astype(str).str.strip().str.replace(r'\\.0+$', '', regex=True)\n",
    "    arm_concepts = set(arm_df[concept_col])\n",
    "\n",
    "    f1_w = compute_weighted_f1(arm_concepts, gold_concepts, record_count_df)\n",
    "    # print(f\"‚úÖ {disease}-{arm}: Weighted F1 = {f1_w}\")\n",
    "    weighted_f1_scores.append(f1_w)\n",
    "\n",
    "# summary_df[\"Weighted_F1\"] = weighted_f1_scores\n",
    "\n",
    "# --------------------------------------------\n",
    "# üíæ SAVE OUTPUT\n",
    "# --------------------------------------------\n",
    "summary_df.to_csv(OUTPUT_PATH, index=False)\n",
    "# print(f\"\\n‚úÖ Saved enriched summary ‚Üí {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase B ‚Äî Vocabulary & Mapping Coverage Analysis (Refined)\n",
    "\n",
    "1. **Standard vs Non-Standard Concepts (FRD 6.2.2 & 6.2.4):** This cell evaluates each workflow‚Äôs included concepts, classifying them as standard or non-standard, and summarizes the counts per arm and disease.  \n",
    "2. **Vocabulary Distribution (FRD 6.2.2):** It quantifies the representation of different source vocabularies (e.g., ICD-10-CM) across arms and diseases to assess coverage and potential gaps.  \n",
    "3. **Mapping Coverage (FRD 6.2.2 & 6.2.4):** Non-standard concepts are compared against mapped standards to compute the proportion successfully mapped; results are saved in `vocabulary_standard_summary.csv`, `vocabulary_distribution_summary.csv`, and `mapping_coverage_summary.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß© Vocabulary & Mapping Coverage Analysis ‚Äî flat ConceptSets\n",
    "# ==========================================================\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"pandas.io.formats.style\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Robust ROOT_DIR auto-detect (same as other scripts)\n",
    "# --------------------------------------------\n",
    "try:\n",
    "    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    SCRIPT_DIR = os.getcwd()\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join(SCRIPT_DIR, '..', 'ConceptSets'))\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    alt_root = os.path.join(SCRIPT_DIR, 'ConceptSets')\n",
    "    if os.path.exists(alt_root):\n",
    "        ROOT_DIR = os.path.abspath(alt_root)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"ConceptSets folder not found relative to script.\")\n",
    "\n",
    "OUT_DIR = os.path.join(SCRIPT_DIR, 'results', 'vocabulary_analysis')\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "PLOT_DIR = os.path.join(OUT_DIR, 'plots')\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------------------\n",
    "def strip_numeric_prefix(name: str) -> str:\n",
    "    \"\"\"Remove leading numeric prefixes like '0152-' or '0031_'.\"\"\"\n",
    "    return re.sub(r'^[\\d\\-\\_]+', '', name).strip()\n",
    "\n",
    "def clean_arm_name(folder_name: str, include_subteams: bool=False):\n",
    "    \"\"\"\n",
    "    Return a standard arm label (AI1, HUMAN, etc.) or None for subteams\n",
    "    if include_subteams=False.\n",
    "    \"\"\"\n",
    "    # Normalize\n",
    "    name = strip_numeric_prefix(folder_name)\n",
    "\n",
    "    # skip ONLINE arms entirely\n",
    "    if re.search(r'ONLINE', name, re.IGNORECASE):\n",
    "        return None\n",
    "\n",
    "    # optionally skip subgroup Sxx\n",
    "    if not include_subteams and re.search(r'\\[S\\d+\\]', name, re.IGNORECASE):\n",
    "        return None\n",
    "\n",
    "    # find standard arm token\n",
    "    m = re.search(r'\\[(AI\\d+|H\\d+|HUMAN|REVIEWER|CLINICIAN|MANUAL|EXPERT)\\]', name, re.IGNORECASE)\n",
    "    if m:\n",
    "        arm = m.group(1).upper()\n",
    "        if arm.startswith('H'):\n",
    "            return 'HUMAN'\n",
    "        return arm\n",
    "\n",
    "    # fallback: any bracketed token (take last)\n",
    "    parts = re.findall(r'\\[([A-Za-z0-9]+)\\]', name)\n",
    "    if parts:\n",
    "        return parts[-1].upper()\n",
    "\n",
    "    # last fallback: use trailing words\n",
    "    toks = re.split(r'[\\s_\\-]+', name)\n",
    "    return toks[-1].upper() if toks else name.upper()\n",
    "\n",
    "def load_csv_safe(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, dtype=str)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # return empty DataFrame to keep downstream logic simple\n",
    "        print(f\"  ‚ö†Ô∏è Failed to read {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def classify_standard_status(df):\n",
    "    if \"standardConcept\" not in df.columns:\n",
    "        return pd.Series([\"Unknown\"] * len(df))\n",
    "    return df[\"standardConcept\"].fillna(\"\").astype(str).str.strip().str.upper().map(lambda x: \"Standard\" if x == \"S\" else \"Non-Standard\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Storage\n",
    "# -------------------------------------------------------------\n",
    "all_standard_stats = []\n",
    "all_vocab_stats = []\n",
    "all_mapping_stats = []\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Main: iterate flat folders directly under ROOT_DIR\n",
    "# -------------------------------------------------------------\n",
    "folders = [f for f in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, f))]\n",
    "# print(f\"Found {len(folders)} folders in ConceptSets (root: {ROOT_DIR})\")\n",
    "\n",
    "for folder in sorted(folders):\n",
    "    folder_path = os.path.join(ROOT_DIR, folder)\n",
    "    clean_name = strip_numeric_prefix(folder)\n",
    "\n",
    "    # skip ONLINE and any hidden folders\n",
    "    if re.search(r'ONLINE', clean_name, re.IGNORECASE):\n",
    "        # print(f\"Skipping ONLINE folder: {folder}\")\n",
    "        continue\n",
    "    # identify disease code, e.g. [C01]\n",
    "    m = re.search(r'\\[(C\\d+)\\]', clean_name, re.IGNORECASE)\n",
    "    if not m:\n",
    "        # skip folders that do not appear to be concept set arms\n",
    "        # print(f\"Skipping non-conceptset folder: {folder}\")\n",
    "        continue\n",
    "    disease_code = m.group(1).upper()\n",
    "\n",
    "    # derive disease display name optionally from the folder text\n",
    "    # pick the trailing disease name after the last bracket if present\n",
    "    disease_display = re.sub(r'^.*\\]', '', clean_name).strip()\n",
    "    arm = clean_arm_name(folder, include_subteams=False)\n",
    "    if arm is None:\n",
    "        continue\n",
    "\n",
    "    # file paths inside this flat folder\n",
    "    included_path = os.path.join(folder_path, \"includedConcepts.csv\")\n",
    "    mapped_path = os.path.join(folder_path, \"mappedConcepts.csv\")\n",
    "\n",
    "    if not os.path.exists(included_path):\n",
    "        # print(f\"  ‚ö†Ô∏è No includedConcepts.csv in {folder} ‚Äî skipping\")\n",
    "        continue\n",
    "\n",
    "    included_df = load_csv_safe(included_path)\n",
    "    mapped_df = load_csv_safe(mapped_path) if os.path.exists(mapped_path) else pd.DataFrame()\n",
    "\n",
    "    # ---- Standard vs Non-Standard summary\n",
    "    if included_df is None or included_df.empty:\n",
    "        continue\n",
    "    included_df[\"StandardStatus\"] = classify_standard_status(included_df)\n",
    "    std_summary = included_df[\"StandardStatus\"].value_counts().rename_axis(\"StandardStatus\").reset_index(name=\"Count\")\n",
    "    std_summary[\"Disease\"] = disease_code\n",
    "    std_summary[\"Arm\"] = arm\n",
    "    std_summary[\"Total\"] = len(included_df)\n",
    "    all_standard_stats.append(std_summary)\n",
    "\n",
    "    # ---- Vocabulary distribution\n",
    "    if \"vocabularyId\" in included_df.columns:\n",
    "        vocab_summary = included_df[\"vocabularyId\"].fillna(\"UNKNOWN\").value_counts().rename_axis(\"vocabularyId\").reset_index(name=\"Count\")\n",
    "        vocab_summary[\"Disease\"] = disease_code\n",
    "        vocab_summary[\"Arm\"] = arm\n",
    "        all_vocab_stats.append(vocab_summary)\n",
    "\n",
    "    # ---- Mapping coverage (non-standard ‚Üí mapped)\n",
    "    # Identify non-standard concept IDs\n",
    "    if \"conceptId\" in included_df.columns:\n",
    "        non_standard_df = included_df[included_df[\"StandardStatus\"] != \"Standard\"]\n",
    "        non_std_ids = set(non_standard_df[\"conceptId\"].dropna().astype(str))\n",
    "    else:\n",
    "        non_std_ids = set()\n",
    "\n",
    "    if not non_std_ids:\n",
    "        # append an explicit record to indicate no non-standard concepts\n",
    "        all_mapping_stats.append({\n",
    "            \"Disease\": disease_code, \"Arm\": arm,\n",
    "            \"Non_Standard_Concepts\": 0,\n",
    "            \"Mapped_to_Standard\": 0,\n",
    "            \"Unmapped\": 0,\n",
    "            \"Mapping_Coverage_%\": None,\n",
    "            \"Folder\": folder\n",
    "        })\n",
    "    else:\n",
    "        mapped_ids = set(mapped_df[\"conceptId\"].dropna().astype(str)) if not mapped_df.empty and \"conceptId\" in mapped_df.columns else set()\n",
    "        mapped_to_standard = non_std_ids & mapped_ids\n",
    "        coverage_pct = round(len(mapped_to_standard) / len(non_std_ids) * 100, 2) if non_std_ids else None\n",
    "        all_mapping_stats.append({\n",
    "            \"Disease\": disease_code, \"Arm\": arm,\n",
    "            \"Non_Standard_Concepts\": len(non_std_ids),\n",
    "            \"Mapped_to_Standard\": len(mapped_to_standard),\n",
    "            \"Unmapped\": len(non_std_ids - mapped_to_standard),\n",
    "            \"Mapping_Coverage_%\": coverage_pct,\n",
    "            \"Folder\": folder\n",
    "        })\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# Combine and save\n",
    "# -------------------------------------------------------------\n",
    "df_standard = pd.concat(all_standard_stats, ignore_index=True) if all_standard_stats else pd.DataFrame()\n",
    "df_vocab = pd.concat(all_vocab_stats, ignore_index=True) if all_vocab_stats else pd.DataFrame()\n",
    "df_mapping = pd.DataFrame(all_mapping_stats) if all_mapping_stats else pd.DataFrame()\n",
    "\n",
    "df_standard.to_csv(os.path.join(OUT_DIR, \"vocabulary_standard_summary.csv\"), index=False)\n",
    "df_vocab.to_csv(os.path.join(OUT_DIR, \"vocabulary_distribution_summary.csv\"), index=False)\n",
    "df_mapping.to_csv(os.path.join(OUT_DIR, \"mapping_coverage_summary.csv\"), index=False)\n",
    "\n",
    "# print(f\"\\n‚úÖ Saved results in: {OUT_DIR}\")\n",
    "# print(\" - vocabulary_standard_summary.csv\")\n",
    "# print(\" - vocabulary_distribution_summary.csv\")\n",
    "# print(\" - mapping_coverage_summary.csv\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üßæ Display Tables\n",
    "# -------------------------------------------------------------\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "\n",
    "# ---------- STANDARD/NON-STANDARD ----------\n",
    "print(\"\\n==============================\")\n",
    "print(\"üß© STANDARD VS NON-STANDARD SUMMARY\")\n",
    "print(\"==============================\")\n",
    "for disease in df_standard[\"Disease\"].unique():\n",
    "    sub = df_standard[df_standard[\"Disease\"] == disease].copy()\n",
    "    pivot = sub.pivot_table(\n",
    "        index=\"Arm\", columns=\"StandardStatus\", values=\"Count\", aggfunc=\"sum\", fill_value=0\n",
    "    ).reset_index()\n",
    "    pivot[\"Total\"] = pivot.sum(axis=1, numeric_only=True)\n",
    "    pivot[\"% Standard\"] = (pivot.get(\"Standard\", 0) / pivot[\"Total\"] * 100).round(1)\n",
    "    print(f\"\\n=== Disease: {disease} ===\")\n",
    "    display(pivot.style.background_gradient(subset=[\"% Standard\"], cmap=\"Blues\").format(precision=1))\n",
    "\n",
    "# ---------- VOCABULARY DISTRIBUTION ----------\n",
    "print(\"\\n==============================\")\n",
    "print(\"üìö VOCABULARY DISTRIBUTION SUMMARY\")\n",
    "print(\"==============================\")\n",
    "for disease in df_vocab[\"Disease\"].unique():\n",
    "    sub = df_vocab[df_vocab[\"Disease\"] == disease].copy()\n",
    "    pivot = sub.pivot_table(index=\"vocabularyId\", columns=\"Arm\", values=\"Count\", aggfunc=\"sum\", fill_value=0)\n",
    "    pivot[\"Total\"] = pivot.sum(axis=1)\n",
    "    pivot = pivot.sort_values(by=\"Total\", ascending=False).drop(columns=\"Total\")\n",
    "    print(f\"\\n=== Disease: {disease} ===\")\n",
    "    display(pivot.style.background_gradient(cmap=\"Purples\").format(precision=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase B ‚Äî Objective 3: Assessing Clinical Impact of Workflow Disagreements (FRD 6.2.2, Objective 3)\n",
    "\n",
    "1. **Metric Computation:** Calculates Spearman‚Äôs rank correlation between Weighted F1 ($F1_W$) and Unweighted F1 across all workflows to quantify the influence of concept prevalence on performance.  \n",
    "2. **Clinical Insight:** Lower correlation values highlight that disagreements disproportionately affect high-prevalence concepts, revealing potential clinical consequences of workflow variability.  \n",
    "3. **Result Display:** Outputs the correlation coefficient and p-value directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==========================================================\n",
    "# # üîç Objective 3 ‚Äî Clinical Impact of Disagreements\n",
    "# # ==========================================================\n",
    "# print(\"\\nüîπ Objective 3: Clinical Impact of Disagreements\")\n",
    "\n",
    "# # Identify the unweighted F1 column (often labeled 'F1')\n",
    "# unweighted_col = \"Unweighted_F1\" if \"Unweighted_F1\" in summary_df.columns else \"F1\"\n",
    "\n",
    "# if all(c in summary_df.columns for c in [\"Weighted_F1\", unweighted_col]):\n",
    "#     spearman_corr, spearman_p = stats.spearmanr(\n",
    "#         summary_df[\"Weighted_F1\"], summary_df[unweighted_col],\n",
    "#         nan_policy='omit'\n",
    "#     )\n",
    "#     print(f\"‚úÖ Spearman‚Äôs œÅ = {spearman_corr:.3f}, p = {spearman_p:.4f}\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è Neither 'F1' nor 'Unweighted_F1' found ‚Äî cannot compute Spearman correlation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Phase B ‚Äî Source Vocabulary Sensitivity Analysis (FRD 6.2.4)\n",
    "\n",
    "This analysis evaluates how different source vocabularies affect the workflow‚Äôs ability to match the True Gold Standard (TGS). It helps identify whether certain vocabularies disproportionately influence AI or Human arm performance.\n",
    "\n",
    "**Procedure:**  \n",
    "1. Load the adjudicated Gold Standard (`*_Gold_Standard_FINAL.csv`) for each disease.  \n",
    "2. Identify all source vocabularies contributing concepts to the Gold Standard.  \n",
    "3. For each arm (Human or AI), compute the weighted performance metrics restricted to each vocabulary subset.  \n",
    "4. Record counts of concepts that were correctly mapped (true positives), missed (false negatives), or erroneously included (false positives).  \n",
    "\n",
    "**Deliverable:**  \n",
    "The combined results for all diseases and arms are saved as `combined_source_vocab_sensitivity.csv`, summarizing the vocabulary-specific Weighted F1, Weighted Precision, Weighted Recall, and weighted counts of TP, FP, and FN.  \n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==========================================================\n",
    "# # üîç Objective 4 ‚Äî Source Vocabulary Sensitivity Analysis\n",
    "# # ==========================================================\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# print(\"\\nüîπ Objective 4: Source Vocabulary Sensitivity Analysis (Standalone)\")\n",
    "\n",
    "# # Load record counts for weighting\n",
    "# record_count_df = pd.read_csv(\"ConceptRecordCounts.csv\", dtype=str)\n",
    "# record_count_df[\"record_count\"] = record_count_df[\"record_count\"].astype(float)\n",
    "\n",
    "# # Identify all FINAL Gold Standard files\n",
    "# final_files = [f for f in os.listdir(\".\") if f.endswith(\"_Gold_Standard_FINAL.csv\")]\n",
    "# print(f\"üìÇ Found {len(final_files)} FINAL gold standard files.\\n\")\n",
    "\n",
    "# all_vocab_results = []\n",
    "\n",
    "# for final_file in final_files:\n",
    "#     disease_folder = final_file.replace(\"_Gold_Standard_FINAL.csv\", \"\")\n",
    "#     if not os.path.exists(disease_folder):\n",
    "#         print(f\"‚ö†Ô∏è Disease folder not found for {disease_folder}\")\n",
    "#         continue\n",
    "\n",
    "#     # Load final adjudicated file\n",
    "#     final_df = pd.read_csv(final_file, dtype=str)\n",
    "#     if \"vocabularyId\" not in final_df.columns:\n",
    "#         print(f\"‚ö†Ô∏è No 'vocabularyId' in {final_file}, skipping.\")\n",
    "#         continue\n",
    "\n",
    "#     vocab_groups = final_df[\"vocabularyId\"].dropna().unique()\n",
    "#     print(f\"=== Disease: {disease_folder} ===\")\n",
    "#     print(f\"   Found vocabularies: {', '.join(vocab_groups)}\")\n",
    "\n",
    "#     # --- Identify all arms for this disease\n",
    "#     arm_data = {}\n",
    "#     for subfolder in os.listdir(disease_folder):\n",
    "#         subfolder_path = os.path.join(disease_folder, subfolder)\n",
    "#         if os.path.isdir(subfolder_path):\n",
    "#             csv_path = os.path.join(subfolder_path, \"includedConcepts.csv\")\n",
    "#             if os.path.exists(csv_path):\n",
    "#                 arm_name = clean_arm_name(subfolder)\n",
    "#                 # ‚úÖ Skip subgroups and duplicates\n",
    "#                 if arm_name is None or arm_name == \"UNKNOWN\" or arm_name in arm_data:\n",
    "#                     continue\n",
    "#                 arm_data[arm_name] = set(load_included_concepts(csv_path))\n",
    "\n",
    "#     # --- Compute vocabulary-specific weighted F1\n",
    "#     for vocab in vocab_groups:\n",
    "#         vocab_gs = set(\n",
    "#             final_df.loc[final_df[\"vocabularyId\"] == vocab, \"conceptId\"]\n",
    "#             .dropna().astype(str)\n",
    "#         )\n",
    "#         for arm, concepts in arm_data.items():\n",
    "#             # compute_weighted_f1 returns dict with F1_W, P_W, etc.\n",
    "#             weighted = compute_weighted_f1(concepts & vocab_gs, vocab_gs, record_count_df)\n",
    "\n",
    "#             all_vocab_results.append({\n",
    "#                 \"Disease\": disease_folder,\n",
    "#                 \"Arm\": arm,\n",
    "#                 \"Vocabulary\": vocab,\n",
    "#                 \"Weighted_F1_Source\": weighted[\"F1_W\"],\n",
    "#                 \"Weighted_Precision_Source\": weighted[\"P_W\"],\n",
    "#                 \"Weighted_Recall_Source\": weighted[\"R_W\"],\n",
    "#                 \"WTP_Source\": weighted[\"WTP\"],\n",
    "#                 \"WFP_Source\": weighted[\"WFP\"],\n",
    "#                 \"WFN_Source\": weighted[\"WFN\"]\n",
    "#             })\n",
    "\n",
    "#             f1_str = f\"{weighted['F1_W']:.3f}\" if weighted[\"F1_W\"] is not None else \"NA\"\n",
    "#             print(f\"     {arm:<5} | {vocab:<20} ‚Üí Weighted_F1_Source={f1_str}\")\n",
    "\n",
    "# print(\"\\n‚úÖ Completed source vocabulary sensitivity analysis for all diseases.\")\n",
    "\n",
    "# # --- Combine all results (deduplicated)\n",
    "# if all_vocab_results:\n",
    "#     vocab_df = pd.DataFrame(all_vocab_results)\n",
    "#     vocab_df = vocab_df.drop_duplicates(subset=[\"Disease\", \"Arm\", \"Vocabulary\"], keep=\"first\")\n",
    "#     vocab_df.to_csv(\"combined_source_vocab_sensitivity.csv\", index=False)\n",
    "#     print(f\"‚úÖ Saved combined file: combined_source_vocab_sensitivity.csv \"\n",
    "#           f\"({vocab_df.shape[0]} unique rows across {vocab_df['Disease'].nunique()} diseases)\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è No vocabulary results generated ‚Äî please check inputs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html --no-input --output \"Phase2_results.html\" \"Phase2.ipynb\" --log-level=ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AllChunks_Combiner",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
